[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi there, Everyone!\nMy official registration name is Deepshika Saravanan. I hold a Bachelor’s degree in Technology in department Computer Science from Chennai, India from SRM Institute of Science and Technology, 2 years ago. My educational goals for this course include developing a deep connection with big data analytics, mastering data analysis techniques, and gaining proficiency in deploying relevant tools. In the broader context of my Mason program of study, I aim to build a strong foundation in data management and mining, as well as enhance my skills in data visualization for analytics. In addition to the current course, I am taking Principles of Data Management/Mining and Visualization for Analytics this semester.\nI have experience working in various computing environments, including both local and cloud-based platforms. Locally, I am proficient in operating systems such as Windows and Linux. In terms of cloud services, I don’t have hands-on experience with AWS (Amazon Web Services) but have done few courses on online for an overview experience of the concepts. My familiarity extends to utilizing cloud resources for storage, computation, and deploying applications in minor projects. While I may not consider myself an expert, I have a solid understanding of cloud computing concepts.\nAt this moment, I don’t have specific concerns about taking this course. However, I am open to addressing any challenges that may arise during the course and am committed to actively engaging with the material to ensure a successful learning experience throughout the course."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deepshika Saravanan",
    "section": "",
    "text": "MSDA Graduate student @ George Mason University | Software Engineer | Former Senior Analyst @ Capgemini Technology Services | Python | SQL | Bigdata Analytics | Machine Learning | Data Visualisation\nWith a solid foundation in Data Science, I have honed my abilities to extract, clean, analyze, and visualize data to derive actionable insights. My experience includes working with diverse datasets, from structured databases to unstructured text, and applying advanced analytics techniques to uncover hidden patterns and trends."
  },
  {
    "objectID": "index.html#my-goal-is-to-leverage-my-passion-for-data-and-my-skills-in-data-science-to-make-a-meaningful-impact.-whether-its-optimizing-business-operations-improving-customer-experiences-or-driving-strategic-decision-making-i-am-dedicated-to-using-data-to-drive-positive-change-and-drive-towards-excellence.",
    "href": "index.html#my-goal-is-to-leverage-my-passion-for-data-and-my-skills-in-data-science-to-make-a-meaningful-impact.-whether-its-optimizing-business-operations-improving-customer-experiences-or-driving-strategic-decision-making-i-am-dedicated-to-using-data-to-drive-positive-change-and-drive-towards-excellence.",
    "title": "Deepshika Saravanan",
    "section": "My goal is to leverage my passion for data and my skills in Data Science to make a meaningful impact. Whether its optimizing business operations, improving customer experiences, or driving strategic decision-making, I am dedicated to using data to drive positive change and drive towards excellence.",
    "text": "My goal is to leverage my passion for data and my skills in Data Science to make a meaningful impact. Whether its optimizing business operations, improving customer experiences, or driving strategic decision-making, I am dedicated to using data to drive positive change and drive towards excellence."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Here is the Youtube link: -\nhttps://youtu.be/oRzrxBoEOOs"
  },
  {
    "objectID": "projects.html#welcome-to-my-stat-515-mid-project",
    "href": "projects.html#welcome-to-my-stat-515-mid-project",
    "title": "Projects",
    "section": "",
    "text": "Here is the Youtube link: -\nhttps://youtu.be/oRzrxBoEOOs"
  },
  {
    "objectID": "projects.html#title-using-r-to-redesign-statistical-graphs-application-to-flight-and-spotify-datasets",
    "href": "projects.html#title-using-r-to-redesign-statistical-graphs-application-to-flight-and-spotify-datasets",
    "title": "Projects",
    "section": "Title: Using R to Redesign Statistical Graphs: Application to Flight and Spotify Datasets",
    "text": "Title: Using R to Redesign Statistical Graphs: Application to Flight and Spotify Datasets\nEffective data visualization is crucial for clearly and successfully expressing ideas in the field of statistical research. In our first STAT 515 mid-project, we used R programming to restyle graphs for two different datasets: Flights and Spotify. Notably, we revised the graphics showing the distribution of airtime and the average delay by month for the Flights dataset and converted the time data into a more understandable format. In a similar vein, we redesigned the Spotify dataset’s popularity numbers and top 10 genres presentation. By making these efforts, we want to improve the graphical representations’ readability and aesthetic appeal and enable more in-depth understanding of the underlying data trends.\nOne of the most effective methods for identifying patterns and trends in datasets is statistical visualization. In order to improve readability and clarity, we redesigned the Flights and Spotify dataset graphs using R programming for our project. Our goal was to give better insights into these statistics by revamping graphs for important metrics including delay, airtime distribution, popularity, and genre distribution, as well as altering the way time data is represented.\nFor data manipulation and visualization, we used R programming and pertinent packages like ggplot2, plotly and lubridate. To make the Flights dataset easier to understand, we converted the departure and arrival timings into the HH:MM format. We also modified the graphs to show the distribution of airtime and the average delay by month. In a similar vein, we recreated graphs for the Spotify dataset in order to show the top 10 genres and the popularity value dispersion. Iterative improvement ensured the visualizations’ efficiency and clarity throughout the process.\nWe were able to learn more about the distribution of airtime among flights and seasonal fluctuations in delays by analyzing the Flights dataset. We effectively communicated these patterns using new graphics, making it possible for to quickly understand the results. In a similar vein, our investigation of the Spotify dataset revealed information about song popularity and the frequency of various musical genres\nWe were able to improve readability and clarity, two issues with the previous visualizations, by revamping the graphs. We enhanced the graphs’ readability and aesthetic appeal by modernizing the way time data was represented and honing the graphical display of important parameters. The significance of proficient data visualization in deriving meaningful conclusions from intricate datasets is emphasized by our project.\nIn conclusion, our mid project STAT 515 project has shown how powerful R programming can be when it comes to creating new statistical graphs for the Flights and Spotify datasets. We improved the graphs’ readability and clarity by modernizing the way time data was represented and enhancing the way important metrics were visualized. This allowed us to get a better understanding of the visualizations. In the future, we hope to investigate more sophisticated visualization methods in order to gain even more profound understanding of intricate datasets.\n#References: -\n#Dataset of Flights:-\nhttps://www.kaggle.com/code/saeedehkamjoo/visualization-on-flights-dataset/notebook\n#Dataset of spotify:-\nhttps://www.kaggle.com/code/ericbiernacki/popular-songs-bad-graph-example/notebook"
  },
  {
    "objectID": "java/index.html",
    "href": "java/index.html",
    "title": "Deepshika Saravanan",
    "section": "",
    "text": "MSDA Graduate student @ George Mason University | Software Engineer | Former Senior Analyst @ Capgemini Technology Services | Python | SQL | Bigdata Analytics | Machine Learning | Data Visualisation\nWith a solid foundation in Data Science, I have honed my abilities to extract, clean, analyze, and visualize data to derive actionable insights. My experience includes working with diverse datasets, from structured databases to unstructured text, and applying advanced analytics techniques to uncover hidden patterns and trends.\nMy goal is to leverage my passion for data and my skills in Data Science to make a meaningful impact. Whether its optimizing business operations, improving customer experiences, or driving strategic decision-making, I am dedicated to using data to drive positive change and drive towards excellence."
  },
  {
    "objectID": "java/about.html",
    "href": "java/about.html",
    "title": "About",
    "section": "",
    "text": "Hi there, Everyone!\nMy official registration name is Deepshika Saravanan. I hold a Bachelor’s degree in Technology in department Computer Science from Chennai, India from SRM Institute of Science and Technology, 2 years ago. My educational goals for this course include developing a deep connection with big data analytics, mastering data analysis techniques, and gaining proficiency in deploying relevant tools. In the broader context of my Mason program of study, I aim to build a strong foundation in data management and mining, as well as enhance my skills in data visualization for analytics. In addition to the current course, I am taking Principles of Data Management/Mining and Visualization for Analytics this semester.\nI have experience working in various computing environments, including both local and cloud-based platforms. Locally, I am proficient in operating systems such as Windows and Linux. In terms of cloud services, I don’t have hands-on experience with AWS (Amazon Web Services) but have done few courses on online for an overview experience of the concepts. My familiarity extends to utilizing cloud resources for storage, computation, and deploying applications in minor projects. While I may not consider myself an expert, I have a solid understanding of cloud computing concepts.\nAt this moment, I don’t have specific concerns about taking this course. However, I am open to addressing any challenges that may arise during the course and am committed to actively engaging with the material to ensure a successful learning experience throughout the course."
  },
  {
    "objectID": "code.html",
    "href": "code.html",
    "title": "code",
    "section": "",
    "text": "The codes which i wrote in R programming language to represent visualizations for STAT-515 MID-PROJECT.Here’s an embedded PDF of Spotify Dataset.:Download PDFHere’s an embedded PDF of Flights Dataset.:Download PDF"
  },
  {
    "objectID": "Code.html",
    "href": "Code.html",
    "title": "Code",
    "section": "",
    "text": "The codes that we wrote in R programming language to represent visualizations for STAT-515 MID-PROJECT.\nHere’s an PDF of Visualisations :Download PDFHere’s an embedded PDF of Code .:Download PDF"
  },
  {
    "objectID": "projects.html#title-using-r-to-redesign-statistical-graphs",
    "href": "projects.html#title-using-r-to-redesign-statistical-graphs",
    "title": "Projects",
    "section": "Title: Using R to Redesign Statistical Graphs:",
    "text": "Title: Using R to Redesign Statistical Graphs:\nData visualization is the graphical representation of data to help people understand the significance of data by summarizing and presenting it in a visual form. It enables decision-makers to see analytics presented visually, so they can grasp difficult concepts or identify new patterns. Through the use of charts, graphs, and maps, data visualization tools provide an accessible way to see and understand trends, outliers, and patterns in data.\nData manipulation refers to the process of changing data to make it easier to read or to prepare it for further analysis. It involves various operations such as sorting, filtering, aggregating, and transforming data to meet specific requirements. Data manipulation is often done using programming languages like Python, R, or SQL, and tools like Excel or pandas. The goal of data manipulation is to ensure that data is in a format that is suitable for analysis and can provide meaningful insights.\nIn R programming, redesigning visualizations is important for several reasons, and there are specific techniques and packages that can be used to achieve this:\nClarity and Effectiveness: R offers a wide range of packages for creating visualizations, such as ggplot2 and plotly, which allow for the creation of clear and effective visualizations through customizable features like color schemes, labels, and annotations.\nAudience Understanding: By using the ggplot2 package in R, visualizations can be customized to suit different audiences. For example, the facet_wrap function can be used to create multiple plots based on different variables, allowing for easy comparison and understanding.\nData Changes: R provides tools for easily updating visualizations with new data. For instance, the update_geom function in ggplot2 can be used to change the data plotted on a graph without having to recreate the entire plot.\nImproved Aesthetics: R offers various themes and styling options through packages like ggthemes, which allow for the creation of visually appealing visualizations. Additionally, the ggplot2 package provides features for customizing plot elements such as fonts, colors, and grid lines.\nNew Insights: Redesigning visualizations in R can lead to the discovery of new insights in the data. For example, by experimenting with different plot types or adding new variables to a plot, new patterns or relationships in the data may become apparent.\nOverall, in R programming, redesigning visualizations is essential for ensuring that they effectively communicate data, are tailored to the audience, and provide meaningful insights. R’s flexibility and extensive visualization capabilities make it a powerful tool for creating and redesigning visualizations to meet these goals.\nThe STAT 515 course provided us with valuable insights into the power of data visualization tools. It highlighted how visualization can effectively communicate complex data and uncover meaningful insights. This course emphasized the importance of using tools like R to create impactful visualizations that can convey information clearly and engage audiences effectively.\n#References: -\n#Dataset of The Top 100 most valuable brands in 2020:-\nhttps://howmuch.net/sources/top-100-most-valuable-brands-2020\n#Dataset of Visualizing the maximum Gender pay gap across USA:-\nhttps://howmuch.net/articles/men-vs-women-comparing-income-by-industry"
  },
  {
    "objectID": "finalproj.html",
    "href": "finalproj.html",
    "title": "Final Project",
    "section": "",
    "text": "EDA OF DATASET :\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(readr)\n\nWarning: package 'readr' was built under R version 4.3.3\n\nlibrary(rpart)\nlibrary(rpart.plot)\n\nWarning: package 'rpart.plot' was built under R version 4.3.3\n\n# Load the dataset\ndata &lt;- read_csv(\"C:\\\\Users\\\\Dell\\\\Downloads\\\\NYSERDA_2023_Soils_Data_for_use_in_the_Large-Scale_Renewables_and_NY-Sun_Programs.csv\")\n\nRows: 8513 Columns: 24\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (16): County, County_MAPSYM, MAPSYM, Multiple MSG Flag, Flag - Fields, C...\ndbl  (8): MUKEY, Default Mineral Soil Group, Flag - MSG Values, Rotation, Co...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Check the structure and summary of data\nstr(data)\n\nspc_tbl_ [8,513 × 24] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ County                    : chr [1:8513] \"Albany\" \"Albany\" \"Albany\" \"Albany\" ...\n $ County_MAPSYM             : chr [1:8513] \"NY001_AE\" \"NY001_ANA\" \"NY001_ANB\" \"NY001_ANC\" ...\n $ MAPSYM                    : chr [1:8513] \"Ae\" \"AnA\" \"AnB\" \"AnC\" ...\n $ MUKEY                     : num [1:8513] 288688 288689 288690 288691 288692 ...\n $ Default Mineral Soil Group: num [1:8513] 7 6 6 7 8 8 9 7 5 5 ...\n $ Multiple MSG Flag         : chr [1:8513] NA NA NA NA ...\n $ Flag - MSG Values         : num [1:8513] NA NA NA NA NA NA NA NA NA NA ...\n $ Flag - Fields             : chr [1:8513] NA NA NA NA ...\n $ Capability Class (FM5 CAP): chr [1:8513] \"4W\" \"3W\" \"3W\" \"3E\" ...\n $ Soil Temp. Regime         : chr [1:8513] \"Mesic\" \"Mesic\" \"Mesic\" \"Mesic\" ...\n $ Soil Modifier             : chr [1:8513] NA NA NA NA ...\n $ Soil Slope                : chr [1:8513] \"\\\"00-03\\\"\" \"\\\"00-03\\\"\" \"\\\"03-08\\\"\" \"\\\"08-15\\\"\" ...\n $ Soil Name                 : chr [1:8513] \"ALLIS\" \"ANGOLA\" \"ANGOLA\" \"ANGOLA\" ...\n $ Drainage                  : chr [1:8513] \"Poorly Drained\" \"Somewhat Well-Drained\" \"Somewhat Well-Drained\" \"Somewhat Well-Drained\" ...\n $ Modifier                  : chr [1:8513] NA NA NA NA ...\n $ Texture                   : chr [1:8513] \"Silty Loam\" \"Silty Loam\" \"Silty Loam\" \"Silty Loam\" ...\n $ Flooding                  : chr [1:8513] NA NA NA NA ...\n $ Lime                      : chr [1:8513] \"Requires lime additions within every rotation\" \"Requires lime additions within every rotation\" \"Requires lime additions within every rotation\" \"Requires lime additions within every rotation\" ...\n $ Rotation                  : num [1:8513] 2 3 3 1 2 3 0 2 4 4 ...\n $ Corn Yield (ton/acre)     : num [1:8513] 7.5 12 12 11.2 6 ...\n $ Hay Yield (ton/acre)      : num [1:8513] 1.6 1.95 1.95 1.6 0.96 0 0 1.6 2.64 2.64 ...\n $ Change                    : chr [1:8513] NA NA NA NA ...\n $ TDN (ton/acre)            : num [1:8513] 0.94 1.4 1.4 0.95 0.48 0 0 1 1.81 1.81 ...\n $ Index (TDN)               : num [1:8513] 20.7 30.9 30.9 20.8 10.6 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   County = col_character(),\n  ..   County_MAPSYM = col_character(),\n  ..   MAPSYM = col_character(),\n  ..   MUKEY = col_double(),\n  ..   `Default Mineral Soil Group` = col_double(),\n  ..   `Multiple MSG Flag` = col_character(),\n  ..   `Flag - MSG Values` = col_double(),\n  ..   `Flag - Fields` = col_character(),\n  ..   `Capability Class (FM5 CAP)` = col_character(),\n  ..   `Soil Temp. Regime` = col_character(),\n  ..   `Soil Modifier` = col_character(),\n  ..   `Soil Slope` = col_character(),\n  ..   `Soil Name` = col_character(),\n  ..   Drainage = col_character(),\n  ..   Modifier = col_character(),\n  ..   Texture = col_character(),\n  ..   Flooding = col_character(),\n  ..   Lime = col_character(),\n  ..   Rotation = col_double(),\n  ..   `Corn Yield (ton/acre)` = col_double(),\n  ..   `Hay Yield (ton/acre)` = col_double(),\n  ..   Change = col_character(),\n  ..   `TDN (ton/acre)` = col_double(),\n  ..   `Index (TDN)` = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n# List unique counties\nunique(data$County)\n\n [1] \"Albany\"        \"Allegany\"      \"Broome\"        \"Cattaraugus\"  \n [5] \"Cayuga\"        \"Chautauqua\"    \"Chemung\"       \"Chenango\"     \n [9] \"Clinton\"       \"Columbia\"      \"Cortland\"      \"Delaware\"     \n[13] \"Dutchess\"      \"Erie\"          \"Essex\"         \"Franklin\"     \n[17] \"Fulton\"        \"Genesee\"       \"Greene\"        \"Hamilton\"     \n[21] \"Herkimer\"      \"Jefferson\"     \"Lewis\"         \"Livingston\"   \n[25] \"Madison\"       \"Monroe\"        \"Montgomery\"    \"Nassau\"       \n[29] \"Niagara\"       \"Oneida\"        \"Onondaga\"      \"Ontario\"      \n[33] \"Orange\"        \"Orleans\"       \"Oswego\"        \"Otsego\"       \n[37] \"Putnam\"        \"Rensselaer\"    \"Rockland\"      \"Saratoga\"     \n[41] \"Schenectady\"   \"Schoharie\"     \"Schuyler\"      \"Seneca\"       \n[45] \"Seneca Nation\" \"St Lawrence\"   \"Steuben\"       \"Suffolk\"      \n[49] \"Sullivan\"      \"Tioga\"         \"Tompkins\"      \"Ulster\"       \n[53] \"Warren\"        \"Washington\"    \"Wayne\"         \"Westchester\"  \n[57] \"Wyoming\"       \"Yates\"\nlibrary(dplyr)\n\n# Calculate average yield for each county\ncounty_avg_yield &lt;- data %&gt;%\n  group_by(County) %&gt;%\n  summarize(Avg_Corn_Yield = mean(`Corn Yield (ton/acre)`, na.rm = TRUE),\n            Avg_Hay_Yield = mean(`Hay Yield (ton/acre)`, na.rm = TRUE))\n\n# View the resulting data frame\nprint(county_avg_yield)\n\n# A tibble: 58 × 3\n   County      Avg_Corn_Yield Avg_Hay_Yield\n   &lt;chr&gt;                &lt;dbl&gt;         &lt;dbl&gt;\n 1 Albany                9.96          1.95\n 2 Allegany              8.55          1.66\n 3 Broome                9.28          1.75\n 4 Cattaraugus           9.39          1.77\n 5 Cayuga               12.2           2.33\n 6 Chautauqua           10.9           2.15\n 7 Chemung              11.9           2.25\n 8 Chenango             11.0           2.21\n 9 Clinton               7.64          1.35\n10 Columbia             10.7           2.12\n# ℹ 48 more rows\nlibrary(ggplot2)\nlibrary(plotly)\n\nWarning: package 'plotly' was built under R version 4.3.3\n\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nlibrary(dplyr)\n\n# Calculate average yield for each county\ncounty_avg_yield &lt;- data %&gt;%\n  group_by(County) %&gt;%\n  summarize(Avg_Corn_Yield = mean(`Corn Yield (ton/acre)`, na.rm = TRUE))\n\n# Create a ggplot object for average corn yields by county\np_corn &lt;- ggplot(data = county_avg_yield, aes(x = reorder(County, -Avg_Corn_Yield), y = Avg_Corn_Yield, fill = Avg_Corn_Yield)) +\n  geom_bar(stat = \"identity\", show.legend = FALSE) +  # Remove legend\n  scale_fill_gradient(low = \"yellow\", high = \"darkorange\") +  # Use a gradient fill from yellow to dark orange\n  labs(title = \"Average Corn Yield by County\", x = \"County\", y = \"Average Yield (ton/acre)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 8),  # Rotate x-axis labels for better fit\n        axis.title = element_text(size = 12, face = \"bold\"))  # Bold and larger axis titles\n\n# Convert the ggplot object to a Plotly object\nplotly_corn &lt;- ggplotly(p_corn)\n\n# Optionally, customize further with layout options in Plotly\nplotly_corn &lt;- layout(plotly_corn,\n                   xaxis = list(title = \"County\"),\n                   yaxis = list(title = \"Average Yield (ton/acre)\"),\n                   title = \"Average Corn Yield by County\")\n\n# Print or render the plot\nplotly_corn\n# Calculate average yield for each county\ncounty_avg_yield &lt;- data %&gt;%\n  group_by(County) %&gt;%\n  summarize(Avg_Hay_Yield = mean(`Hay Yield (ton/acre)`, na.rm = TRUE))\n\n# Create a ggplot object for average hay yields by county\np_hay &lt;- ggplot(data = county_avg_yield, aes(x = reorder(County, -Avg_Hay_Yield), y = Avg_Hay_Yield, fill = Avg_Hay_Yield)) +\n  geom_bar(stat = \"identity\", show.legend = FALSE) +  # Remove legend\n  scale_fill_gradient(low = \"lightgreen\", high = \"darkgreen\") +  # Use a gradient fill from light green to dark green\n  labs(title = \"Average Hay Yield by County\", x = \"County\", y = \"Average Yield (ton/acre)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 8),  # Rotate x-axis labels for better fit\n        axis.title = element_text(size = 12, face = \"bold\"))  # Bold and larger axis titles\n\n# Convert the ggplot object to a Plotly object\nplotly_hay &lt;- ggplotly(p_hay)\n\n# Optionally, customize further with layout options in Plotly\nplotly_hay &lt;- layout(plotly_hay,\n                   xaxis = list(title = \"County\"),\n                   yaxis = list(title = \"Average Yield (ton/acre)\"),\n                   title = \"Average Hay Yield by County\")\n\n# Print or render the plot\nplotly_hay\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(plotly)\n\n# Calculate average yields for each county\ncounty_avg_yield &lt;- data %&gt;%\n  group_by(County) %&gt;%\n  summarise(\n    Average_Corn_Yield = mean(`Corn Yield (ton/acre)`, na.rm = TRUE),\n    Average_Hay_Yield = mean(`Hay Yield (ton/acre)`, na.rm = TRUE),\n    Total_Yield = Average_Corn_Yield + Average_Hay_Yield\n  ) %&gt;%\n  arrange(Total_Yield)  # Arrange by total average yield in ascending order\n\n# Create hover text\ncounty_avg_yield &lt;- county_avg_yield %&gt;%\n  mutate(Hover_Info = sprintf(\"County: %s\\nCorn Yield: %.2f ton/acre\\nHay Yield: %.2f ton/acre\\nTotal Yield: %.2f ton/acre\",\n                              County, Average_Corn_Yield, Average_Hay_Yield, Total_Yield))\n\n# Create the ggplot object with hover info\np_stacked &lt;- ggplot(data = county_avg_yield, aes(x = reorder(County, Total_Yield), text = Hover_Info)) +\n  geom_bar(aes(y = Average_Corn_Yield), stat = \"identity\", fill = \"orange\") +\n  geom_bar(aes(y = -Average_Hay_Yield), stat = \"identity\", fill = \"lightgreen\") +\n  labs(title = \"Stacked Average Yields by County\",\n       x = \"County\", y = \"Average Yield (ton/acre)\", fill = \"Crop Type\") +\n  scale_y_continuous(labels = abs) +\n  scale_fill_manual(values = c(\"orange\", \"lightgreen\"), labels = c(\"Corn\", \"Hay\")) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 8))\n\n# Convert to Plotly and add custom hover info\nplotly_stacked &lt;- ggplotly(p_stacked, tooltip = \"text\")\n\nplotly_stacked &lt;- layout(plotly_stacked,\n                         xaxis = list(title = \"County\"),\n                         yaxis = list(title = \"Yield (ton/acre)\"),\n                         title = \"Interactive: Stacked Average Yields by County\")\nplotly_stacked\nAverage Corn Yield by County (First Graph):\nThis bar graph represents the average corn yield for various counties, arranged in descending order. Each bar reflects the yield of corn in tons per acre for a specific county.\nKey Observations:\nYield Variability: There is a significant variability in corn yields among counties. The graph shows a stark contrast between the highest-yielding counties and those at the lower end. Counties with yields over 10 tons per acre contrast sharply with those that barely produce 1 ton per acre.\nGeographical Clustering: The highest-yielding counties are grouped on the left side of the graph. This clustering might suggest geographical or climatic advantages, or it could reflect areas with more advanced farming techniques and better soil management practices.\nAverage Hay Yield by County (Second Graph):\nSimilar to the first, this graph displays the average hay yield in tons per acre across different counties, arranged in descending order.\nKey Observations:\nMore Uniform Yields: Compared to corn, hay yields across counties show a more uniform distribution. The highest yields are not as extreme, and the decrease across counties appears more gradual.\nLower Maximum Yields: The highest hay yields are significantly lower than the highest corn yields, which might reflect different market values, crop demands, or inherent differences in how these crops are cultivated and harvested.\nResearch Question 1: What is the effect of soil texture on hay yield?\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(plotly)\n\n# Load your dataset\ndata &lt;- read.csv(\"C:\\\\Users\\\\Dell\\\\Downloads\\\\NYSERDA_2023_Soils_Data_for_use_in_the_Large-Scale_Renewables_and_NY-Sun_Programs.csv\")\n\n# Ensure that the Texture variable is correctly formatted as a factor\ndata$Texture &lt;- as.factor(data$Texture)\n\n# Fit a linear regression model to see the effect of Soil Texture on Hay Yield\nmodel_texture_hay &lt;- lm(Hay.Yield..ton.acre. ~ Texture, data = data)\n\n# Summary of the model to check for significance and coefficients\nsummary(model_texture_hay)\n\n\nCall:\nlm(formula = Hay.Yield..ton.acre. ~ Texture, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.3525 -0.9993  0.1368  0.8999  5.9249 \n\nCoefficients:\n                                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                        1.466430   0.027733  52.876  &lt; 2e-16 ***\nTextureClay                        0.599999   0.448901   1.337 0.181390    \nTextureClay Loam                  -0.204763   0.684959  -0.299 0.764992    \nTextureFine Sand                  -0.076630   0.238699  -0.321 0.748195    \nTextureFine Sandy Loam             0.496004   0.049579  10.004  &lt; 2e-16 ***\nTextureFlaggery                    0.133570   0.838671   0.159 0.873464    \nTextureLoam                        0.476070   0.593354   0.802 0.422380    \nTextureLoamy Fine Sand             0.604302   0.075268   8.029 1.12e-15 ***\nTextureLoamy Sand                  0.001250   0.073221   0.017 0.986379    \nTextureLoamy Very Fine Sand        0.343570   0.484736   0.709 0.478482    \nTextureMuck                        0.021723   0.087700   0.248 0.804375    \nTextureMuck/Peat                   0.158570   0.484736   0.327 0.743580    \nTexturePeat                       -1.000383   0.182888  -5.470 4.63e-08 ***\nTextureSand                       -0.009699   0.234127  -0.041 0.966957    \nTextureSandy Clay Loam             0.843570   0.838671   1.006 0.314520    \nTextureSandy Loam                  0.269538   0.080220   3.360 0.000783 ***\nTextureSilty Clay                  0.301728   0.273362   1.104 0.269726    \nTextureSilty Clay Loam             0.492861   0.075043   6.568 5.41e-11 ***\nTextureSilty Loam                  0.648634   0.033245  19.511  &lt; 2e-16 ***\nTextureSilty Loam, Non-Calcareous -0.506430   1.185735  -0.427 0.669316    \nTextureVery Fine Sandy Loam        0.886070   0.089098   9.945  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.185 on 8492 degrees of freedom\nMultiple R-squared:  0.0622,    Adjusted R-squared:  0.05999 \nF-statistic: 28.16 on 20 and 8492 DF,  p-value: &lt; 2.2e-16\n\n# Optional: Plot the relationship\np &lt;- ggplot(data, aes(x = Texture, y = Hay.Yield..ton.acre.)) +\n  geom_boxplot() +\n  labs(title = \"Impact of Soil Texture on Hay Yield\",\n       x = \"Soil Texture\",\n       y = \"Hay Yield (ton/acre)\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Convert ggplot to Plotly\nplotly_plot &lt;- ggplotly(p)\nprint(plotly_plot)\nInterpretation: Variability in Yield: The boxplot indicates significant variability in hay yield across different soil textures. Some textures, like “Loamy Sand” and “Fine Sand,” show a broader range of yields, indicating that other factors may influence the yield in addition to the texture. Median Yield Differences: Textures such as “Silty Loam” and “Loam” appear to have higher median yields compared to more coarse textures like “Sandy Loam” or “Fine Sandy Loam.” This could be due to better water retention capabilities of finer-textured soils. Outliers: There are notable outliers in several categories, such as “Silty Loam” and “Loamy Sand.” These outliers may represent specific conditions or errors in data that could significantly influence the average yield calculations. Further Analysis Suggestions: Statistical Tests: Conduct statistical tests (like ANOVA) to formally test if the differences in yields across soil textures are statistically significant.\n# Load the necessary library\nlibrary(dplyr)\n\n# Assuming your data is already loaded into a DataFrame named data\n# Make sure to convert Texture to a factor if it isn't already\ndata$Texture &lt;- as.factor(data$Texture)\n\n# Perform the ANOVA\nanova_result &lt;- aov(Hay.Yield..ton.acre. ~ Texture, data = data)\n\n# Display the summary of the ANOVA test\nsummary(anova_result)\n\n              Df Sum Sq Mean Sq F value Pr(&gt;F)    \nTexture       20    791   39.57   28.16 &lt;2e-16 ***\nResiduals   8492  11933    1.41                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nHere’s a breakdown of the results:\nDegrees of Freedom (Df): Texture: 20 (reflecting the 21 different categories of soil texture, with 20 degrees of freedom for the comparisons). Residuals: 8492 (reflecting the leftover degrees of freedom after accounting for the factor levels). Sum of Squares: Sum Sq (Texture): 791 (the total variation attributable to differences between different soil textures). Sum Sq (Residuals): 11933 (the variation within the groups of the same texture, i.e., the error or residual variation). Mean Squares: Mean Sq (Texture): 39.57 (the average variation between groups; calculated as Sum Sq divided by Df for texture). Mean Sq (Residuals): 1.41 (the average variation within groups; calculated as Sum Sq divided by Df for residuals). F value: 28.16 (a measure of the ratio of the variance between the groups to the variance within the groups, which is a significant statistic in determining if the means of different groups are statistically different). Pr(&gt;F): &lt; 2e-16 (the p-value indicating the probability of observing such a large F statistic if the null hypothesis is true, i.e., all group means are the same. A value less than 0.05, here very close to zero, indicates strong evidence against the null hypothesis). Conclusion: Given the very low p-value (&lt; 2e-16), you can reject the null hypothesis and conclude that there are statistically significant differences in hay yield among different soil textures. This suggests that soil texture significantly impacts hay yield. With such a result, further investigation into which specific textures contribute most to these differences might be warranted, possibly using post-hoc tests to compare individual group means pairwise."
  },
  {
    "objectID": "finalproj.html#quarto",
    "href": "finalproj.html#quarto",
    "title": "Final Project",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "finalproj.html#running-code",
    "href": "finalproj.html#running-code",
    "title": "Final Project",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "quarto/projects.html",
    "href": "quarto/projects.html",
    "title": "Projects",
    "section": "",
    "text": "Here is the Youtube link: -\nhttps://youtu.be/oRzrxBoEOOs"
  },
  {
    "objectID": "quarto/projects.html#welcome-to-my-stat-515-mid-project",
    "href": "quarto/projects.html#welcome-to-my-stat-515-mid-project",
    "title": "Projects",
    "section": "",
    "text": "Here is the Youtube link: -\nhttps://youtu.be/oRzrxBoEOOs"
  },
  {
    "objectID": "quarto/projects.html#title-using-r-to-redesign-statistical-graphs",
    "href": "quarto/projects.html#title-using-r-to-redesign-statistical-graphs",
    "title": "Projects",
    "section": "Title: Using R to Redesign Statistical Graphs:",
    "text": "Title: Using R to Redesign Statistical Graphs:\nData visualization is the graphical representation of data to help people understand the significance of data by summarizing and presenting it in a visual form. It enables decision-makers to see analytics presented visually, so they can grasp difficult concepts or identify new patterns. Through the use of charts, graphs, and maps, data visualization tools provide an accessible way to see and understand trends, outliers, and patterns in data.\nData manipulation refers to the process of changing data to make it easier to read or to prepare it for further analysis. It involves various operations such as sorting, filtering, aggregating, and transforming data to meet specific requirements. Data manipulation is often done using programming languages like Python, R, or SQL, and tools like Excel or pandas. The goal of data manipulation is to ensure that data is in a format that is suitable for analysis and can provide meaningful insights.\nIn R programming, redesigning visualizations is important for several reasons, and there are specific techniques and packages that can be used to achieve this:\nClarity and Effectiveness: R offers a wide range of packages for creating visualizations, such as ggplot2 and plotly, which allow for the creation of clear and effective visualizations through customizable features like color schemes, labels, and annotations.\nAudience Understanding: By using the ggplot2 package in R, visualizations can be customized to suit different audiences. For example, the facet_wrap function can be used to create multiple plots based on different variables, allowing for easy comparison and understanding.\nData Changes: R provides tools for easily updating visualizations with new data. For instance, the update_geom function in ggplot2 can be used to change the data plotted on a graph without having to recreate the entire plot.\nImproved Aesthetics: R offers various themes and styling options through packages like ggthemes, which allow for the creation of visually appealing visualizations. Additionally, the ggplot2 package provides features for customizing plot elements such as fonts, colors, and grid lines.\nNew Insights: Redesigning visualizations in R can lead to the discovery of new insights in the data. For example, by experimenting with different plot types or adding new variables to a plot, new patterns or relationships in the data may become apparent.\nOverall, in R programming, redesigning visualizations is essential for ensuring that they effectively communicate data, are tailored to the audience, and provide meaningful insights. R’s flexibility and extensive visualization capabilities make it a powerful tool for creating and redesigning visualizations to meet these goals.\nThe STAT 515 course provided us with valuable insights into the power of data visualization tools. It highlighted how visualization can effectively communicate complex data and uncover meaningful insights. This course emphasized the importance of using tools like R to create impactful visualizations that can convey information clearly and engage audiences effectively.\n#References: -\n#Dataset of The Top 100 most valuable brands in 2020:-\nhttps://howmuch.net/sources/top-100-most-valuable-brands-2020\n#Dataset of Visualizing the maximum Gender pay gap across USA:-\nhttps://howmuch.net/articles/men-vs-women-comparing-income-by-industry"
  },
  {
    "objectID": "Midproject/projects.html",
    "href": "Midproject/projects.html",
    "title": "Projects",
    "section": "",
    "text": "Here is the Youtube link: -"
  },
  {
    "objectID": "Midproject/projects.html#welcome-to-my-stat-515-mid-project",
    "href": "Midproject/projects.html#welcome-to-my-stat-515-mid-project",
    "title": "Projects",
    "section": "",
    "text": "Here is the Youtube link: -"
  },
  {
    "objectID": "Midproject/projects.html#title-using-r-to-redesign-statistical-graphs",
    "href": "Midproject/projects.html#title-using-r-to-redesign-statistical-graphs",
    "title": "Projects",
    "section": "Title: Using R to Redesign Statistical Graphs:",
    "text": "Title: Using R to Redesign Statistical Graphs:\nData visualization is the graphical representation of data to help people understand the significance of data by summarizing and presenting it in a visual form. It enables decision-makers to see analytics presented visually, so they can grasp difficult concepts or identify new patterns. Through the use of charts, graphs, and maps, data visualization tools provide an accessible way to see and understand trends, outliers, and patterns in data.\nData manipulation refers to the process of changing data to make it easier to read or to prepare it for further analysis. It involves various operations such as sorting, filtering, aggregating, and transforming data to meet specific requirements. Data manipulation is often done using programming languages like Python, R, or SQL, and tools like Excel or pandas. The goal of data manipulation is to ensure that data is in a format that is suitable for analysis and can provide meaningful insights.\nIn R programming, redesigning visualizations is important for several reasons, and there are specific techniques and packages that can be used to achieve this:\nClarity and Effectiveness: R offers a wide range of packages for creating visualizations, such as ggplot2 and plotly, which allow for the creation of clear and effective visualizations through customizable features like color schemes, labels, and annotations.\nAudience Understanding: By using the ggplot2 package in R, visualizations can be customized to suit different audiences. For example, the facet_wrap function can be used to create multiple plots based on different variables, allowing for easy comparison and understanding.\nData Changes: R provides tools for easily updating visualizations with new data. For instance, the update_geom function in ggplot2 can be used to change the data plotted on a graph without having to recreate the entire plot.\nImproved Aesthetics: R offers various themes and styling options through packages like ggthemes, which allow for the creation of visually appealing visualizations. Additionally, the ggplot2 package provides features for customizing plot elements such as fonts, colors, and grid lines.\nNew Insights: Redesigning visualizations in R can lead to the discovery of new insights in the data. For example, by experimenting with different plot types or adding new variables to a plot, new patterns or relationships in the data may become apparent.\nOverall, in R programming, redesigning visualizations is essential for ensuring that they effectively communicate data, are tailored to the audience, and provide meaningful insights. R’s flexibility and extensive visualization capabilities make it a powerful tool for creating and redesigning visualizations to meet these goals.\nThe STAT 515 course provided us with valuable insights into the power of data visualization tools. It highlighted how visualization can effectively communicate complex data and uncover meaningful insights. This course emphasized the importance of using tools like R to create impactful visualizations that can convey information clearly and engage audiences effectively.\n#References: -\n#Dataset of The Top 100 most valuable brands in 2020:-\nhttps://howmuch.net/sources/top-100-most-valuable-brands-2020\n#Dataset of Visualizing the maximum Gender pay gap across USA:-\nhttps://howmuch.net/articles/men-vs-women-comparing-income-by-industry"
  },
  {
    "objectID": "finalprojcode.html",
    "href": "finalprojcode.html",
    "title": "Final Project Code",
    "section": "",
    "text": "Show the code\n# Load necessary library\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nShow the code\n# Read the dataset\ndata &lt;- read.csv(\"C:\\\\Users\\\\Dell\\\\Downloads\\\\NYSERDA_2023_Soils_Data_for_use_in_the_Large-Scale_Renewables_and_NY-Sun_Programs.csv\")\n\n# Convert factors to numeric if necessary (assuming 'Flooding' is binary or categorical)\ndata$Drainage &lt;- as.numeric(as.factor(data$Drainage))\ndata$Flooding &lt;- as.numeric(as.factor(data$Flooding))\n\n# Handling NA values\ndata &lt;- na.omit(data)\n\n# Using cor.test to determine the correlation\ncor_test &lt;- cor.test(data$Drainage, data$Flooding)\nprint(cor_test)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  data$Drainage and data$Flooding\nt = 16.142, df = 407, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5618257 0.6804973\nsample estimates:\n      cor \n0.6247561 \n\n\n\n\n\nShow the code\n# Load necessary libraries\nlibrary(randomForest)\n\n\nWarning: package 'randomForest' was built under R version 4.3.3\n\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\nShow the code\nlibrary(dplyr)\ncolnames(data)\n\n\n [1] \"County\"                     \"County_MAPSYM\"             \n [3] \"MAPSYM\"                     \"MUKEY\"                     \n [5] \"Default.Mineral.Soil.Group\" \"Multiple.MSG.Flag\"         \n [7] \"Flag...MSG.Values\"          \"Flag...Fields\"             \n [9] \"Capability.Class..FM5.CAP.\" \"Soil.Temp..Regime\"         \n[11] \"Soil.Modifier\"              \"Soil.Slope\"                \n[13] \"Soil.Name\"                  \"Drainage\"                  \n[15] \"Modifier\"                   \"Texture\"                   \n[17] \"Flooding\"                   \"Lime\"                      \n[19] \"Rotation\"                   \"Corn.Yield..ton.acre.\"     \n[21] \"Hay.Yield..ton.acre.\"       \"Change\"                    \n[23] \"TDN..ton.acre.\"             \"Index..TDN.\"               \n\n\nShow the code\n# Handling NA values - assuming you're interested in predicting 'Flooding'\ndata_clean &lt;- na.omit(data[, c(\"Flooding\", \"Drainage\", \"Texture\", \"Soil.Slope\", \"Capability.Class..FM5.CAP.\")])\ndata_clean$Texture &lt;- as.numeric(as.factor(data_clean$Texture))\n\n\n\n# Convert all categorical variables to factor type\ndata_clean$Capability_Class &lt;- as.factor(data_clean$\"Capability.Class..FM5.CAP.\")\n\n\n# Fit Random Forest model\nset.seed(123)  # for reproducibility\nrf_model &lt;- randomForest(Flooding ~ ., data=data_clean, ntree=500, importance=TRUE)\n\n\nWarning in randomForest.default(m, y, ...): The response has five or fewer\nunique values.  Are you sure you want to do regression?\n\n\nShow the code\n# Print model summary\nprint(rf_model)\n\n\n\nCall:\n randomForest(formula = Flooding ~ ., data = data_clean, ntree = 500,      importance = TRUE) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n\n          Mean of squared residuals: 0.08768223\n                    % Var explained: 91.48\n\n\nShow the code\n# Plot importance of variables\nvarImpPlot(rf_model)\n\n\n\n\n\n\n\n\n\nYou can add options to executable code like this\n\n\nShow the code\n# Load necessary library\nlibrary(stats)\n\ndata$Texture &lt;- as.numeric(as.factor(data$Texture))  # Convert categorical to numeric\ndata$Drainage &lt;- as.numeric(as.factor(data$Drainage))  # Convert categorical to numeric if needed\n\n# Fit Multiple Regression Model with Interaction Term\nmodel_interaction &lt;- lm(Flooding ~ Drainage * Texture, data = data)\n\n# Summary of the model to see coefficients and significance\nsummary(model_interaction)\n\n\n\nCall:\nlm(formula = Flooding ~ Drainage * Texture, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0282 -0.2530 -0.1270  0.3186  2.4724 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -0.349160   0.133186  -2.622  0.00908 ** \nDrainage          0.229149   0.015232  15.044  &lt; 2e-16 ***\nTexture           0.294757   0.020929  14.084  &lt; 2e-16 ***\nDrainage:Texture -0.020150   0.002235  -9.018  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6614 on 405 degrees of freedom\nMultiple R-squared:  0.5791,    Adjusted R-squared:  0.576 \nF-statistic: 185.8 on 3 and 405 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nShow the code\n# Load necessary libraries\nlibrary(randomForest)\nlibrary(dplyr)\n\n# Prepare the data\ndata &lt;- mutate(data, Drainage_Texture_Interaction = Drainage * Texture)\n\n# Fit Random Forest Model including the engineered interaction feature\nset.seed(123)  # for reproducibility\nrf_model_interaction &lt;- randomForest(Flooding ~ Drainage + Texture + Drainage_Texture_Interaction, data = data, ntree = 500)\n\n\nWarning in randomForest.default(m, y, ...): The response has five or fewer\nunique values.  Are you sure you want to do regression?\n\n\nShow the code\n# Evaluate the model (assuming Flooding is continuous; adjust as necessary)\nprint(rf_model_interaction)\n\n\n\nCall:\n randomForest(formula = Flooding ~ Drainage + Texture + Drainage_Texture_Interaction,      data = data, ntree = 500) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n\n          Mean of squared residuals: 0.1969851\n                    % Var explained: 80.86\n\n\nShow the code\n# Optionally, view the importance of the new interaction feature\nimportance(rf_model_interaction)\n\n\n                             IncNodePurity\nDrainage                          115.0117\nTexture                           121.6274\nDrainage_Texture_Interaction      131.4226\n\n\n\n\nShow the code\n# Load necessary libraries\nlibrary(caret)\n\n\nWarning: package 'caret' was built under R version 4.3.3\n\n\nLoading required package: ggplot2\n\n\n\nAttaching package: 'ggplot2'\n\n\nThe following object is masked from 'package:randomForest':\n\n    margin\n\n\nLoading required package: lattice\n\n\nShow the code\nlibrary(randomForest)\n\ndata$Texture &lt;- as.numeric(as.factor(data$Texture))\ndata$Drainage &lt;- as.numeric(as.factor(data$Drainage))\n\n# Define training control\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)  # 10-fold cross-validation\n\n# Fit Linear Regression Model using cross-validation\nlm_model_cv &lt;- train(Flooding ~ Drainage + Texture + Drainage:Texture, data = data, method = \"lm\", trControl = train_control)\n\n# Fit Random Forest Model using cross-validation\nrf_model_cv &lt;- train(Flooding ~ Drainage + Texture + Drainage:Texture, data = data, method = \"rf\", trControl = train_control, ntree = 500)\n\n\nnote: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .\n\n\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\n\n\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\n\n\nShow the code\n# Summary of cross-validation results\nprint(lm_model_cv)\n\n\nLinear Regression \n\n409 samples\n  2 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 367, 368, 368, 368, 369, 367, ... \nResampling results:\n\n  RMSE       Rsquared   MAE      \n  0.6608545  0.5871187  0.4143339\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\nShow the code\nprint(rf_model_cv)\n\n\nRandom Forest \n\n409 samples\n  2 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 368, 369, 368, 369, 368, 367, ... \nResampling results across tuning parameters:\n\n  mtry  RMSE       Rsquared   MAE      \n  2     0.4282331  0.8134469  0.1653817\n  3     0.4379896  0.8039825  0.1682584\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was mtry = 2."
  },
  {
    "objectID": "finalprojcode.html#quarto",
    "href": "finalprojcode.html#quarto",
    "title": "Final Project Code",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "finalprojcode.html#running-code",
    "href": "finalprojcode.html#running-code",
    "title": "Final Project Code",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "finalproj.html#research-question-2-is-there-a-correlation-between-the-drainage-of-the-soil-and-the-presence-of-flooding",
    "href": "finalproj.html#research-question-2-is-there-a-correlation-between-the-drainage-of-the-soil-and-the-presence-of-flooding",
    "title": "Final Project",
    "section": "",
    "text": "The Pearson correlation coefficient between the ‘Drainage’ and ‘Flooding’ columns in your dataset is approximately 0.625, indicating a moderate positive correlation. This suggests that as drainage characteristics of the soil increase (presumably indicating better drainage capacity), the occurrence of flooding also increases. This result might seem counterintuitive at first glance; however, it could indicate that areas classified with better drainage are also those where water accumulates quickly, thus making them prone to flooding under certain conditions.\n\nInterpretation and Next Steps: Statistical Significance: The p-value is less than 0.05 (actually, it’s much smaller than that), indicating that the correlation is statistically significant, and the likelihood that this correlation is due to random chance is very low. Practical Significance: While statistically significant, the strength of the correlation (moderate) suggests that while there is a relationship, other factors also significantly influence the presence of flooding. It’s important to consider these factors in any risk assessment or land use planning. Further Analysis: Investigating how other variables interact with ‘Drainage’ and ‘Flooding’ could provide deeper insights. For instance, soil texture, slope, and capability class could be influencing how quickly water is absorbed or runs off, affecting flooding.\n\nThese plots are key to understanding which predictors are most influential in modeling the outcome (in your case, flooding).\nInterpretation of the Variable Importance Plots: %IncMSE: This plot shows the increase in Mean Squared Error (MSE) of the model when each variable is randomly shuffled. A higher value indicates that the model relies more on that variable for prediction, meaning the variable is more important. According to your graph, ‘Drainage’ seems to be the most important predictor, followed by ‘Capability_Class’ and ‘Texture’. IncNodePurity: This measure is based on the total decrease in node impurities from splitting on the variable, averaged over all trees. Node impurity is typically measured by the RSS (regression) or Gini impurity (classification). In this graph, ‘Capability_Class’ contributes most to node purity, followed by ‘Drainage’ and ‘Texture’. This suggests that ‘Capability_Class’ is particularly effective at creating homogeneous nodes, likely due to its role in determining soil usability.\n#Investigating interaction effects between predictors like ‘Drainage’ and ‘Texture’ in the context of their impact on flooding can provide deeper insights into how these variables jointly influence the outcome. In R, you can include interaction terms directly in your model formula to study these effects. Here, we’ll look at two approaches: using multiple regression to evaluate the statistical significance of the interaction, and using a Random Forest model to assess the predictive power when interactions are considered.\nR Code for Multiple Regression with Interaction Terms We’ll modify the linear regression model to include an interaction term between ‘Drainage’ and ‘Texture’. This will allow us to see if the effect of one variable on flooding depends on the level of the other variable.\n\nInterpretation of the Regression Output Coefficients: Intercept (-0.349160): This represents the baseline value of flooding when both ‘Drainage’ and ‘Texture’ are at their reference levels (typically zero in numerical coding). Drainage (0.229149): This coefficient indicates that for each unit increase in drainage (without considering the impact of texture), flooding increases by approximately 0.229 units, holding other factors constant. Texture (0.294757): Similarly, for each unit increase in texture, flooding increases by approximately 0.295 units, holding other factors constant.\nDrainage:Texture Interaction (-0.020150): The negative interaction term suggests that the combined effect of ‘Drainage’ and ‘Texture’ on flooding is less than the sum of their individual effects. In other words, higher levels of one may slightly mitigate the influence of the other on flooding. Statistical Significance: All predictors, including the interaction term, are highly statistically significant (p &lt; 0.001), indicating strong evidence against the null hypothesis of no effect. Model Fit: Residual Standard Error (RSE) (0.6614): This measures the typical size of the residuals, and in your context, it implies that the standard deviation of the residuals is around 0.661 units. Multiple R-squared (0.5791): Approximately 57.91% of the variability in flooding is explained by the model, which is a decent level of explanatory power for natural science data. Adjusted R-squared (0.576): Slightly adjusted for the number of predictors, still indicating a good fit. Residuals: The spread of residuals suggests that while the model fits well for many observations (median close to zero), there are outliers and some predictions that deviate significantly from the actual values, as indicated by the min and max residuals.\nRandom Forest inherently considers interactions among features, but we can explicitly engineer an interaction feature to see how it influences model performance.\n\nInterpretation of the Random Forest Model Results: Model Performance: Mean of squared residuals (0.1969851): This value is considerably lower than the residual standard error from the linear regression model (0.6614), suggesting that the Random Forest model has better predictive accuracy. % Var explained (80.86%): A high percentage of variance explained indicates that the Random Forest model is effectively capturing the relationships and variability in the data. It explains more than 80% of the variance in flooding, which is significantly higher than the Multiple R-squared from the linear regression model (57.91%). Feature Importance: Drainage: Importance score of 115.0117, suggesting it’s a significant predictor of flooding. Texture: Importance score of 121.6274, slightly more influential than ‘Drainage’. Drainage_Texture_Interaction: With the highest importance score of 131.4226, this engineered feature seems to be the most significant predictor in the model. This underscores the value of including interaction terms explicitly, even in a model like Random Forest that inherently accounts for interactions among features. Comparison and Conclusion: Explanatory Power: The linear regression model provides clear coefficients that describe the relationship between each predictor and the outcome, including how the interaction term modifies these relationships. This is particularly useful for hypothesis testing and understanding the specific effects of changes in predictors. Predictive Accuracy: The Random Forest model outperforms the linear regression in terms of predictive accuracy, explaining a higher percentage of the variance in flooding and producing a lower mean squared residual. Feature Importance: Random Forest offers an advantage in evaluating the importance of features, including interactions, without needing a specific hypothesis about their effects.\n\nCross-Validation for Linear Regression and Random Forest\n\n\nThe cross-validation results for both the Linear Regression and Random Forest models provide a clear comparison of their performance:\nLinear Regression Model: RMSE (Root Mean Squared Error): 0.6608545 R-squared: 0.5871187 MAE (Mean Absolute Error): 0.4143339 These metrics indicate that the linear regression model explains about 58.71% of the variance in the data. The RMSE and MAE values provide a measure of the average error in the predictions.\nRandom Forest Model: RMSE: 0.4282331 R-squared: 0.8134469 MAE: 0.1653817 Optimal mtry: 2 The Random Forest model significantly outperforms the Linear Regression in all the metrics. It explains approximately 81.34% of the variance in the data, and both its RMSE and MAE are lower, indicating more accurate predictions.\n\nThe diagnostic plots for your linear regression model provide valuable insights into how well the model meets the assumptions necessary for optimal performance. Here’s an interpretation of each plot:\n\n\nResiduals vs Fitted This plot helps check for non-linearity and heteroscedasticity (unequal variance of residuals).\n\nObservations: The residuals do not appear to display any clear pattern, which is good for linearity. However, there is a slight “fanning” effect where the spread of residuals increases with fitted values, suggesting potential heteroscedasticity. Action: Consider transformations of the dependent variable or use heteroscedasticity-consistent standard errors if this model will be used for inferential purposes. 2. Normal Q-Q This plot shows if the residuals are normally distributed—a key assumption of linear regression.\nObservations: Most points lie on the line, but there are deviations at the tails (both lower and upper ends), indicating slight departures from normality. Action: This is generally not severe unless very precise estimates are required. For more robustness, consider using non-parametric bootstrapping techniques to estimate standard errors. 3. Scale-Location (or Spread-Location) This plot checks if residuals are spread equally along the ranges of predictors (homoscedasticity).\nObservations: The red line (a loess fit) shows a trend, which suggests that residuals have non-constant variance across the range of fitted values. Action: This supports the earlier suggestion of possible heteroscedasticity. Transformations or robust regression methods might be needed. 4. Residuals vs Leverage This plot helps to identify influential cases that might have an unduly large effect on the model estimate.\nObservations: Most data points have low leverage, but there are a few points well outside the Cook’s distance lines (notably the points labeled 7280 and 4430). Action: Investigate these points further to determine if they are outliers or influential points due to data entry errors or other reasons. Consider removing or adjusting these points if they are deemed to be errors. Conclusion The diagnostic plots indicate that while the model does not suffer from severe issues, there are indications of potential heteroscedasticity and some influence from outliers or high-leverage points."
  },
  {
    "objectID": "influence.html",
    "href": "influence.html",
    "title": "test",
    "section": "",
    "text": "# Load necessary library\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# Read the dataset\ndata &lt;- read.csv(\"C:\\\\Users\\\\Dell\\\\Downloads\\\\NYSERDA_2023_Soils_Data_for_use_in_the_Large-Scale_Renewables_and_NY-Sun_Programs.csv\")\n\n# Convert factors to numeric if necessary (assuming 'Flooding' is binary or categorical)\ndata$Drainage &lt;- as.numeric(as.factor(data$Drainage))\ndata$Flooding &lt;- as.numeric(as.factor(data$Flooding))\n\n# Handling NA values\ndata &lt;- na.omit(data)\n\n# Using cor.test to determine the correlation\ncor_test &lt;- cor.test(data$Drainage, data$Flooding)\nprint(cor_test)\n\n\n    Pearson's product-moment correlation\n\ndata:  data$Drainage and data$Flooding\nt = 16.142, df = 407, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5618257 0.6804973\nsample estimates:\n      cor \n0.6247561 \n\n\nThe Pearson correlation coefficient between the ‘Drainage’ and ‘Flooding’ columns in your dataset is approximately 0.625, indicating a moderate positive correlation. This suggests that as drainage characteristics of the soil increase (presumably indicating better drainage capacity), the occurrence of flooding also increases. This result might seem counterintuitive at first glance; however, it could indicate that areas classified with better drainage are also those where water accumulates quickly, thus making them prone to flooding under certain conditions.\nInterpretation and Next Steps: Statistical Significance: The p-value is less than 0.05 (actually, it’s much smaller than that), indicating that the correlation is statistically significant, and the likelihood that this correlation is due to random chance is very low. Practical Significance: While statistically significant, the strength of the correlation (moderate) suggests that while there is a relationship, other factors also significantly influence the presence of flooding. It’s important to consider these factors in any risk assessment or land use planning. Further Analysis: Investigating how other variables interact with ‘Drainage’ and ‘Flooding’ could provide deeper insights. For instance, soil texture, slope, and capability class could be influencing how quickly water is absorbed or runs off, affecting flooding.\n\n# Load necessary libraries\nlibrary(randomForest)\n\nWarning: package 'randomForest' was built under R version 4.3.3\n\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nlibrary(dplyr)\ncolnames(data)\n\n [1] \"County\"                     \"County_MAPSYM\"             \n [3] \"MAPSYM\"                     \"MUKEY\"                     \n [5] \"Default.Mineral.Soil.Group\" \"Multiple.MSG.Flag\"         \n [7] \"Flag...MSG.Values\"          \"Flag...Fields\"             \n [9] \"Capability.Class..FM5.CAP.\" \"Soil.Temp..Regime\"         \n[11] \"Soil.Modifier\"              \"Soil.Slope\"                \n[13] \"Soil.Name\"                  \"Drainage\"                  \n[15] \"Modifier\"                   \"Texture\"                   \n[17] \"Flooding\"                   \"Lime\"                      \n[19] \"Rotation\"                   \"Corn.Yield..ton.acre.\"     \n[21] \"Hay.Yield..ton.acre.\"       \"Change\"                    \n[23] \"TDN..ton.acre.\"             \"Index..TDN.\"               \n\n# Handling NA values - assuming you're interested in predicting 'Flooding'\ndata_clean &lt;- na.omit(data[, c(\"Flooding\", \"Drainage\", \"Texture\", \"Soil.Slope\", \"Capability.Class..FM5.CAP.\")])\ndata_clean$Texture &lt;- as.numeric(as.factor(data_clean$Texture))\n\n\n\n# Convert all categorical variables to factor type\ndata_clean$Capability_Class &lt;- as.factor(data_clean$\"Capability.Class..FM5.CAP.\")\n\n\n# Fit Random Forest model\nset.seed(123)  # for reproducibility\nrf_model &lt;- randomForest(Flooding ~ ., data=data_clean, ntree=500, importance=TRUE)\n\nWarning in randomForest.default(m, y, ...): The response has five or fewer\nunique values.  Are you sure you want to do regression?\n\n# Print model summary\nprint(rf_model)\n\n\nCall:\n randomForest(formula = Flooding ~ ., data = data_clean, ntree = 500,      importance = TRUE) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n\n          Mean of squared residuals: 0.08768223\n                    % Var explained: 91.48\n\n# Plot importance of variables\nvarImpPlot(rf_model)\n\n\n\n\n\n\n\n\nThese plots are key to understanding which predictors are most influential in modeling the outcome (in your case, flooding).\nInterpretation of the Variable Importance Plots: %IncMSE: This plot shows the increase in Mean Squared Error (MSE) of the model when each variable is randomly shuffled. A higher value indicates that the model relies more on that variable for prediction, meaning the variable is more important. According to your graph, ‘Drainage’ seems to be the most important predictor, followed by ‘Capability_Class’ and ‘Texture’. IncNodePurity: This measure is based on the total decrease in node impurities from splitting on the variable, averaged over all trees. Node impurity is typically measured by the RSS (regression) or Gini impurity (classification). In this graph, ‘Capability_Class’ contributes most to node purity, followed by ‘Drainage’ and ‘Texture’. This suggests that ‘Capability_Class’ is particularly effective at creating homogeneous nodes, likely due to its role in determining soil usability.\n#Investigating interaction effects between predictors like ‘Drainage’ and ‘Texture’ in the context of their impact on flooding can provide deeper insights into how these variables jointly influence the outcome. In R, you can include interaction terms directly in your model formula to study these effects. Here, we’ll look at two approaches: using multiple regression to evaluate the statistical significance of the interaction, and using a Random Forest model to assess the predictive power when interactions are considered.\nR Code for Multiple Regression with Interaction Terms We’ll modify the linear regression model to include an interaction term between ‘Drainage’ and ‘Texture’. This will allow us to see if the effect of one variable on flooding depends on the level of the other variable.\n\n# Load necessary library\nlibrary(stats)\n\ndata$Texture &lt;- as.numeric(as.factor(data$Texture))  # Convert categorical to numeric\ndata$Drainage &lt;- as.numeric(as.factor(data$Drainage))  # Convert categorical to numeric if needed\n\n# Fit Multiple Regression Model with Interaction Term\nmodel_interaction &lt;- lm(Flooding ~ Drainage * Texture, data = data)\n\n# Summary of the model to see coefficients and significance\nsummary(model_interaction)\n\n\nCall:\nlm(formula = Flooding ~ Drainage * Texture, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0282 -0.2530 -0.1270  0.3186  2.4724 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -0.349160   0.133186  -2.622  0.00908 ** \nDrainage          0.229149   0.015232  15.044  &lt; 2e-16 ***\nTexture           0.294757   0.020929  14.084  &lt; 2e-16 ***\nDrainage:Texture -0.020150   0.002235  -9.018  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6614 on 405 degrees of freedom\nMultiple R-squared:  0.5791,    Adjusted R-squared:  0.576 \nF-statistic: 185.8 on 3 and 405 DF,  p-value: &lt; 2.2e-16\n\n\nInterpretation of the Regression Output Coefficients: Intercept (-0.349160): This represents the baseline value of flooding when both ‘Drainage’ and ‘Texture’ are at their reference levels (typically zero in numerical coding). Drainage (0.229149): This coefficient indicates that for each unit increase in drainage (without considering the impact of texture), flooding increases by approximately 0.229 units, holding other factors constant. Texture (0.294757): Similarly, for each unit increase in texture, flooding increases by approximately 0.295 units, holding other factors constant. Drainage:Texture Interaction (-0.020150): The negative interaction term suggests that the combined effect of ‘Drainage’ and ‘Texture’ on flooding is less than the sum of their individual effects. In other words, higher levels of one may slightly mitigate the influence of the other on flooding. Statistical Significance: All predictors, including the interaction term, are highly statistically significant (p &lt; 0.001), indicating strong evidence against the null hypothesis of no effect. Model Fit: Residual Standard Error (RSE) (0.6614): This measures the typical size of the residuals, and in your context, it implies that the standard deviation of the residuals is around 0.661 units. Multiple R-squared (0.5791): Approximately 57.91% of the variability in flooding is explained by the model, which is a decent level of explanatory power for natural science data. Adjusted R-squared (0.576): Slightly adjusted for the number of predictors, still indicating a good fit. Residuals: The spread of residuals suggests that while the model fits well for many observations (median close to zero), there are outliers and some predictions that deviate significantly from the actual values, as indicated by the min and max residuals.\n#R Code for Random Forest with Feature Engineering Random Forest inherently considers interactions among features, but we can explicitly engineer an interaction feature to see how it influences model performance.\n\n# Load necessary libraries\nlibrary(randomForest)\nlibrary(dplyr)\n\n# Prepare the data\ndata &lt;- mutate(data, Drainage_Texture_Interaction = Drainage * Texture)\n\n# Fit Random Forest Model including the engineered interaction feature\nset.seed(123)  # for reproducibility\nrf_model_interaction &lt;- randomForest(Flooding ~ Drainage + Texture + Drainage_Texture_Interaction, data = data, ntree = 500)\n\nWarning in randomForest.default(m, y, ...): The response has five or fewer\nunique values.  Are you sure you want to do regression?\n\n# Evaluate the model (assuming Flooding is continuous; adjust as necessary)\nprint(rf_model_interaction)\n\n\nCall:\n randomForest(formula = Flooding ~ Drainage + Texture + Drainage_Texture_Interaction,      data = data, ntree = 500) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n\n          Mean of squared residuals: 0.1969851\n                    % Var explained: 80.86\n\n# Optionally, view the importance of the new interaction feature\nimportance(rf_model_interaction)\n\n                             IncNodePurity\nDrainage                          115.0117\nTexture                           121.6274\nDrainage_Texture_Interaction      131.4226\n\n\nInterpretation of the Random Forest Model Results: Model Performance: Mean of squared residuals (0.1969851): This value is considerably lower than the residual standard error from the linear regression model (0.6614), suggesting that the Random Forest model has better predictive accuracy. % Var explained (80.86%): A high percentage of variance explained indicates that the Random Forest model is effectively capturing the relationships and variability in the data. It explains more than 80% of the variance in flooding, which is significantly higher than the Multiple R-squared from the linear regression model (57.91%). Feature Importance: Drainage: Importance score of 115.0117, suggesting it’s a significant predictor of flooding. Texture: Importance score of 121.6274, slightly more influential than ‘Drainage’. Drainage_Texture_Interaction: With the highest importance score of 131.4226, this engineered feature seems to be the most significant predictor in the model. This underscores the value of including interaction terms explicitly, even in a model like Random Forest that inherently accounts for interactions among features. Comparison and Conclusion: Explanatory Power: The linear regression model provides clear coefficients that describe the relationship between each predictor and the outcome, including how the interaction term modifies these relationships. This is particularly useful for hypothesis testing and understanding the specific effects of changes in predictors. Predictive Accuracy: The Random Forest model outperforms the linear regression in terms of predictive accuracy, explaining a higher percentage of the variance in flooding and producing a lower mean squared residual. Feature Importance: Random Forest offers an advantage in evaluating the importance of features, including interactions, without needing a specific hypothesis about their effects."
  },
  {
    "objectID": "influence.html#r-markdown",
    "href": "influence.html#r-markdown",
    "title": "test",
    "section": "",
    "text": "# Load necessary library\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# Read the dataset\ndata &lt;- read.csv(\"C:\\\\Users\\\\Dell\\\\Downloads\\\\NYSERDA_2023_Soils_Data_for_use_in_the_Large-Scale_Renewables_and_NY-Sun_Programs.csv\")\n\n# Convert factors to numeric if necessary (assuming 'Flooding' is binary or categorical)\ndata$Drainage &lt;- as.numeric(as.factor(data$Drainage))\ndata$Flooding &lt;- as.numeric(as.factor(data$Flooding))\n\n# Handling NA values\ndata &lt;- na.omit(data)\n\n# Using cor.test to determine the correlation\ncor_test &lt;- cor.test(data$Drainage, data$Flooding)\nprint(cor_test)\n\n\n    Pearson's product-moment correlation\n\ndata:  data$Drainage and data$Flooding\nt = 16.142, df = 407, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5618257 0.6804973\nsample estimates:\n      cor \n0.6247561 \n\n\nThe Pearson correlation coefficient between the ‘Drainage’ and ‘Flooding’ columns in your dataset is approximately 0.625, indicating a moderate positive correlation. This suggests that as drainage characteristics of the soil increase (presumably indicating better drainage capacity), the occurrence of flooding also increases. This result might seem counterintuitive at first glance; however, it could indicate that areas classified with better drainage are also those where water accumulates quickly, thus making them prone to flooding under certain conditions.\nInterpretation and Next Steps: Statistical Significance: The p-value is less than 0.05 (actually, it’s much smaller than that), indicating that the correlation is statistically significant, and the likelihood that this correlation is due to random chance is very low. Practical Significance: While statistically significant, the strength of the correlation (moderate) suggests that while there is a relationship, other factors also significantly influence the presence of flooding. It’s important to consider these factors in any risk assessment or land use planning. Further Analysis: Investigating how other variables interact with ‘Drainage’ and ‘Flooding’ could provide deeper insights. For instance, soil texture, slope, and capability class could be influencing how quickly water is absorbed or runs off, affecting flooding.\n\n# Load necessary libraries\nlibrary(randomForest)\n\nWarning: package 'randomForest' was built under R version 4.3.3\n\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nlibrary(dplyr)\ncolnames(data)\n\n [1] \"County\"                     \"County_MAPSYM\"             \n [3] \"MAPSYM\"                     \"MUKEY\"                     \n [5] \"Default.Mineral.Soil.Group\" \"Multiple.MSG.Flag\"         \n [7] \"Flag...MSG.Values\"          \"Flag...Fields\"             \n [9] \"Capability.Class..FM5.CAP.\" \"Soil.Temp..Regime\"         \n[11] \"Soil.Modifier\"              \"Soil.Slope\"                \n[13] \"Soil.Name\"                  \"Drainage\"                  \n[15] \"Modifier\"                   \"Texture\"                   \n[17] \"Flooding\"                   \"Lime\"                      \n[19] \"Rotation\"                   \"Corn.Yield..ton.acre.\"     \n[21] \"Hay.Yield..ton.acre.\"       \"Change\"                    \n[23] \"TDN..ton.acre.\"             \"Index..TDN.\"               \n\n# Handling NA values - assuming you're interested in predicting 'Flooding'\ndata_clean &lt;- na.omit(data[, c(\"Flooding\", \"Drainage\", \"Texture\", \"Soil.Slope\", \"Capability.Class..FM5.CAP.\")])\ndata_clean$Texture &lt;- as.numeric(as.factor(data_clean$Texture))\n\n\n\n# Convert all categorical variables to factor type\ndata_clean$Capability_Class &lt;- as.factor(data_clean$\"Capability.Class..FM5.CAP.\")\n\n\n# Fit Random Forest model\nset.seed(123)  # for reproducibility\nrf_model &lt;- randomForest(Flooding ~ ., data=data_clean, ntree=500, importance=TRUE)\n\nWarning in randomForest.default(m, y, ...): The response has five or fewer\nunique values.  Are you sure you want to do regression?\n\n# Print model summary\nprint(rf_model)\n\n\nCall:\n randomForest(formula = Flooding ~ ., data = data_clean, ntree = 500,      importance = TRUE) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n\n          Mean of squared residuals: 0.08768223\n                    % Var explained: 91.48\n\n# Plot importance of variables\nvarImpPlot(rf_model)\n\n\n\n\n\n\n\n\nThese plots are key to understanding which predictors are most influential in modeling the outcome (in your case, flooding).\nInterpretation of the Variable Importance Plots: %IncMSE: This plot shows the increase in Mean Squared Error (MSE) of the model when each variable is randomly shuffled. A higher value indicates that the model relies more on that variable for prediction, meaning the variable is more important. According to your graph, ‘Drainage’ seems to be the most important predictor, followed by ‘Capability_Class’ and ‘Texture’. IncNodePurity: This measure is based on the total decrease in node impurities from splitting on the variable, averaged over all trees. Node impurity is typically measured by the RSS (regression) or Gini impurity (classification). In this graph, ‘Capability_Class’ contributes most to node purity, followed by ‘Drainage’ and ‘Texture’. This suggests that ‘Capability_Class’ is particularly effective at creating homogeneous nodes, likely due to its role in determining soil usability.\n#Investigating interaction effects between predictors like ‘Drainage’ and ‘Texture’ in the context of their impact on flooding can provide deeper insights into how these variables jointly influence the outcome. In R, you can include interaction terms directly in your model formula to study these effects. Here, we’ll look at two approaches: using multiple regression to evaluate the statistical significance of the interaction, and using a Random Forest model to assess the predictive power when interactions are considered.\nR Code for Multiple Regression with Interaction Terms We’ll modify the linear regression model to include an interaction term between ‘Drainage’ and ‘Texture’. This will allow us to see if the effect of one variable on flooding depends on the level of the other variable.\n\n# Load necessary library\nlibrary(stats)\n\ndata$Texture &lt;- as.numeric(as.factor(data$Texture))  # Convert categorical to numeric\ndata$Drainage &lt;- as.numeric(as.factor(data$Drainage))  # Convert categorical to numeric if needed\n\n# Fit Multiple Regression Model with Interaction Term\nmodel_interaction &lt;- lm(Flooding ~ Drainage * Texture, data = data)\n\n# Summary of the model to see coefficients and significance\nsummary(model_interaction)\n\n\nCall:\nlm(formula = Flooding ~ Drainage * Texture, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0282 -0.2530 -0.1270  0.3186  2.4724 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -0.349160   0.133186  -2.622  0.00908 ** \nDrainage          0.229149   0.015232  15.044  &lt; 2e-16 ***\nTexture           0.294757   0.020929  14.084  &lt; 2e-16 ***\nDrainage:Texture -0.020150   0.002235  -9.018  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6614 on 405 degrees of freedom\nMultiple R-squared:  0.5791,    Adjusted R-squared:  0.576 \nF-statistic: 185.8 on 3 and 405 DF,  p-value: &lt; 2.2e-16\n\n\nInterpretation of the Regression Output Coefficients: Intercept (-0.349160): This represents the baseline value of flooding when both ‘Drainage’ and ‘Texture’ are at their reference levels (typically zero in numerical coding). Drainage (0.229149): This coefficient indicates that for each unit increase in drainage (without considering the impact of texture), flooding increases by approximately 0.229 units, holding other factors constant. Texture (0.294757): Similarly, for each unit increase in texture, flooding increases by approximately 0.295 units, holding other factors constant. Drainage:Texture Interaction (-0.020150): The negative interaction term suggests that the combined effect of ‘Drainage’ and ‘Texture’ on flooding is less than the sum of their individual effects. In other words, higher levels of one may slightly mitigate the influence of the other on flooding. Statistical Significance: All predictors, including the interaction term, are highly statistically significant (p &lt; 0.001), indicating strong evidence against the null hypothesis of no effect. Model Fit: Residual Standard Error (RSE) (0.6614): This measures the typical size of the residuals, and in your context, it implies that the standard deviation of the residuals is around 0.661 units. Multiple R-squared (0.5791): Approximately 57.91% of the variability in flooding is explained by the model, which is a decent level of explanatory power for natural science data. Adjusted R-squared (0.576): Slightly adjusted for the number of predictors, still indicating a good fit. Residuals: The spread of residuals suggests that while the model fits well for many observations (median close to zero), there are outliers and some predictions that deviate significantly from the actual values, as indicated by the min and max residuals.\n#R Code for Random Forest with Feature Engineering Random Forest inherently considers interactions among features, but we can explicitly engineer an interaction feature to see how it influences model performance.\n\n# Load necessary libraries\nlibrary(randomForest)\nlibrary(dplyr)\n\n# Prepare the data\ndata &lt;- mutate(data, Drainage_Texture_Interaction = Drainage * Texture)\n\n# Fit Random Forest Model including the engineered interaction feature\nset.seed(123)  # for reproducibility\nrf_model_interaction &lt;- randomForest(Flooding ~ Drainage + Texture + Drainage_Texture_Interaction, data = data, ntree = 500)\n\nWarning in randomForest.default(m, y, ...): The response has five or fewer\nunique values.  Are you sure you want to do regression?\n\n# Evaluate the model (assuming Flooding is continuous; adjust as necessary)\nprint(rf_model_interaction)\n\n\nCall:\n randomForest(formula = Flooding ~ Drainage + Texture + Drainage_Texture_Interaction,      data = data, ntree = 500) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n\n          Mean of squared residuals: 0.1969851\n                    % Var explained: 80.86\n\n# Optionally, view the importance of the new interaction feature\nimportance(rf_model_interaction)\n\n                             IncNodePurity\nDrainage                          115.0117\nTexture                           121.6274\nDrainage_Texture_Interaction      131.4226\n\n\nInterpretation of the Random Forest Model Results: Model Performance: Mean of squared residuals (0.1969851): This value is considerably lower than the residual standard error from the linear regression model (0.6614), suggesting that the Random Forest model has better predictive accuracy. % Var explained (80.86%): A high percentage of variance explained indicates that the Random Forest model is effectively capturing the relationships and variability in the data. It explains more than 80% of the variance in flooding, which is significantly higher than the Multiple R-squared from the linear regression model (57.91%). Feature Importance: Drainage: Importance score of 115.0117, suggesting it’s a significant predictor of flooding. Texture: Importance score of 121.6274, slightly more influential than ‘Drainage’. Drainage_Texture_Interaction: With the highest importance score of 131.4226, this engineered feature seems to be the most significant predictor in the model. This underscores the value of including interaction terms explicitly, even in a model like Random Forest that inherently accounts for interactions among features. Comparison and Conclusion: Explanatory Power: The linear regression model provides clear coefficients that describe the relationship between each predictor and the outcome, including how the interaction term modifies these relationships. This is particularly useful for hypothesis testing and understanding the specific effects of changes in predictors. Predictive Accuracy: The Random Forest model outperforms the linear regression in terms of predictive accuracy, explaining a higher percentage of the variance in flooding and producing a lower mean squared residual. Feature Importance: Random Forest offers an advantage in evaluating the importance of features, including interactions, without needing a specific hypothesis about their effects."
  },
  {
    "objectID": "finalprojcode.html#research-question-2-is-there-a-correlation-between-the-drainage-of-the-soil-and-the-presence-of-flooding",
    "href": "finalprojcode.html#research-question-2-is-there-a-correlation-between-the-drainage-of-the-soil-and-the-presence-of-flooding",
    "title": "Final Project Code",
    "section": "Research Question 2 : Is there a correlation between the drainage of the soil and the presence of flooding?",
    "text": "Research Question 2 : Is there a correlation between the drainage of the soil and the presence of flooding?\n\n\nShow the code\n# Load necessary library\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nShow the code\n# Read the dataset\ndata &lt;- read.csv(\"C:\\\\Users\\\\Dell\\\\Downloads\\\\NYSERDA_2023_Soils_Data_for_use_in_the_Large-Scale_Renewables_and_NY-Sun_Programs.csv\")\n\n# Convert factors to numeric if necessary (assuming 'Flooding' is binary or categorical)\ndata$Drainage &lt;- as.numeric(as.factor(data$Drainage))\ndata$Flooding &lt;- as.numeric(as.factor(data$Flooding))\n\n# Handling NA values\ndata &lt;- na.omit(data)\n\n# Using cor.test to determine the correlation\ncor_test &lt;- cor.test(data$Drainage, data$Flooding)\nprint(cor_test)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  data$Drainage and data$Flooding\nt = 16.142, df = 407, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5618257 0.6804973\nsample estimates:\n      cor \n0.6247561 \n\n\n\n\n\nShow the code\n# Load necessary libraries\nlibrary(randomForest)\n\n\nWarning: package 'randomForest' was built under R version 4.3.3\n\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\nShow the code\nlibrary(dplyr)\ncolnames(data)\n\n\n [1] \"County\"                     \"County_MAPSYM\"             \n [3] \"MAPSYM\"                     \"MUKEY\"                     \n [5] \"Default.Mineral.Soil.Group\" \"Multiple.MSG.Flag\"         \n [7] \"Flag...MSG.Values\"          \"Flag...Fields\"             \n [9] \"Capability.Class..FM5.CAP.\" \"Soil.Temp..Regime\"         \n[11] \"Soil.Modifier\"              \"Soil.Slope\"                \n[13] \"Soil.Name\"                  \"Drainage\"                  \n[15] \"Modifier\"                   \"Texture\"                   \n[17] \"Flooding\"                   \"Lime\"                      \n[19] \"Rotation\"                   \"Corn.Yield..ton.acre.\"     \n[21] \"Hay.Yield..ton.acre.\"       \"Change\"                    \n[23] \"TDN..ton.acre.\"             \"Index..TDN.\"               \n\n\nShow the code\n# Handling NA values - assuming you're interested in predicting 'Flooding'\ndata_clean &lt;- na.omit(data[, c(\"Flooding\", \"Drainage\", \"Texture\", \"Soil.Slope\", \"Capability.Class..FM5.CAP.\")])\ndata_clean$Texture &lt;- as.numeric(as.factor(data_clean$Texture))\n\n\n\n# Convert all categorical variables to factor type\ndata_clean$Capability_Class &lt;- as.factor(data_clean$\"Capability.Class..FM5.CAP.\")\n\n\n# Fit Random Forest model\nset.seed(123)  # for reproducibility\nrf_model &lt;- randomForest(Flooding ~ ., data=data_clean, ntree=500, importance=TRUE)\n\n\nWarning in randomForest.default(m, y, ...): The response has five or fewer\nunique values.  Are you sure you want to do regression?\n\n\nShow the code\n# Print model summary\nprint(rf_model)\n\n\n\nCall:\n randomForest(formula = Flooding ~ ., data = data_clean, ntree = 500,      importance = TRUE) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n\n          Mean of squared residuals: 0.08768223\n                    % Var explained: 91.48\n\n\nShow the code\n# Plot importance of variables\nvarImpPlot(rf_model)\n\n\n\n\n\n\n\n\n\nYou can add options to executable code like this\n\n\nShow the code\n# Load necessary library\nlibrary(stats)\n\ndata$Texture &lt;- as.numeric(as.factor(data$Texture))  # Convert categorical to numeric\ndata$Drainage &lt;- as.numeric(as.factor(data$Drainage))  # Convert categorical to numeric if needed\n\n# Fit Multiple Regression Model with Interaction Term\nmodel_interaction &lt;- lm(Flooding ~ Drainage * Texture, data = data)\n\n# Summary of the model to see coefficients and significance\nsummary(model_interaction)\n\n\n\nCall:\nlm(formula = Flooding ~ Drainage * Texture, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0282 -0.2530 -0.1270  0.3186  2.4724 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -0.349160   0.133186  -2.622  0.00908 ** \nDrainage          0.229149   0.015232  15.044  &lt; 2e-16 ***\nTexture           0.294757   0.020929  14.084  &lt; 2e-16 ***\nDrainage:Texture -0.020150   0.002235  -9.018  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6614 on 405 degrees of freedom\nMultiple R-squared:  0.5791,    Adjusted R-squared:  0.576 \nF-statistic: 185.8 on 3 and 405 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nShow the code\n# Load necessary libraries\nlibrary(randomForest)\nlibrary(dplyr)\n\n# Prepare the data\ndata &lt;- mutate(data, Drainage_Texture_Interaction = Drainage * Texture)\n\n# Fit Random Forest Model including the engineered interaction feature\nset.seed(123)  # for reproducibility\nrf_model_interaction &lt;- randomForest(Flooding ~ Drainage + Texture + Drainage_Texture_Interaction, data = data, ntree = 500)\n\n\nWarning in randomForest.default(m, y, ...): The response has five or fewer\nunique values.  Are you sure you want to do regression?\n\n\nShow the code\n# Evaluate the model (assuming Flooding is continuous; adjust as necessary)\nprint(rf_model_interaction)\n\n\n\nCall:\n randomForest(formula = Flooding ~ Drainage + Texture + Drainage_Texture_Interaction,      data = data, ntree = 500) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n\n          Mean of squared residuals: 0.1969851\n                    % Var explained: 80.86\n\n\nShow the code\n# Optionally, view the importance of the new interaction feature\nimportance(rf_model_interaction)\n\n\n                             IncNodePurity\nDrainage                          115.0117\nTexture                           121.6274\nDrainage_Texture_Interaction      131.4226\n\n\n\n\nShow the code\n# Load necessary libraries\nlibrary(caret)\n\n\nWarning: package 'caret' was built under R version 4.3.3\n\n\nLoading required package: ggplot2\n\n\n\nAttaching package: 'ggplot2'\n\n\nThe following object is masked from 'package:randomForest':\n\n    margin\n\n\nLoading required package: lattice\n\n\nShow the code\nlibrary(randomForest)\n\ndata$Texture &lt;- as.numeric(as.factor(data$Texture))\ndata$Drainage &lt;- as.numeric(as.factor(data$Drainage))\n\n# Define training control\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)  # 10-fold cross-validation\n\n# Fit Linear Regression Model using cross-validation\nlm_model_cv &lt;- train(Flooding ~ Drainage + Texture + Drainage:Texture, data = data, method = \"lm\", trControl = train_control)\n\n# Fit Random Forest Model using cross-validation\nrf_model_cv &lt;- train(Flooding ~ Drainage + Texture + Drainage:Texture, data = data, method = \"rf\", trControl = train_control, ntree = 500)\n\n\nnote: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .\n\n\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\n\n\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\n\n\nShow the code\n# Summary of cross-validation results\nprint(lm_model_cv)\n\n\nLinear Regression \n\n409 samples\n  2 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 367, 368, 368, 368, 369, 367, ... \nResampling results:\n\n  RMSE       Rsquared   MAE      \n  0.6608545  0.5871187  0.4143339\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\nShow the code\nprint(rf_model_cv)\n\n\nRandom Forest \n\n409 samples\n  2 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 368, 369, 368, 369, 368, 367, ... \nResampling results across tuning parameters:\n\n  mtry  RMSE       Rsquared   MAE      \n  2     0.4282331  0.8134469  0.1653817\n  3     0.4379896  0.8039825  0.1682584\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was mtry = 2."
  },
  {
    "objectID": "finalprojcode.html#research-question-3-how-does-the-presence-of-multiple-mineral-soil-group-multiple-msg-flag-affect-corn-and-hay-yield-across-different-counties",
    "href": "finalprojcode.html#research-question-3-how-does-the-presence-of-multiple-mineral-soil-group-multiple-msg-flag-affect-corn-and-hay-yield-across-different-counties",
    "title": "Final Project Code",
    "section": "Research Question 3: How does the presence of multiple mineral soil group (Multiple MSG Flag) affect corn and hay yield across different counties?",
    "text": "Research Question 3: How does the presence of multiple mineral soil group (Multiple MSG Flag) affect corn and hay yield across different counties?\n\n\nShow the code\n# Load necessary libraries\nlibrary(readr)  # For reading CSV files\n\n\nWarning: package 'readr' was built under R version 4.3.3\n\n\nShow the code\nlibrary(dplyr)  # For data manipulation\nlibrary(ggplot2)  # For creating visualizations\n\n# dataset\ndata = read_csv(\"C:\\\\Users\\\\Dell\\\\Downloads\\\\NYSERDA_2023_Soils_Data_for_use_in_the_Large-Scale_Renewables_and_NY-Sun_Programs.csv\")\n\n\nRows: 8513 Columns: 24\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (16): County, County_MAPSYM, MAPSYM, Multiple MSG Flag, Flag - Fields, C...\ndbl  (8): MUKEY, Default Mineral Soil Group, Flag - MSG Values, Rotation, Co...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nShow the code\n# Check all column names in the dataset\ncolnames(data)\n\n\n [1] \"County\"                     \"County_MAPSYM\"             \n [3] \"MAPSYM\"                     \"MUKEY\"                     \n [5] \"Default Mineral Soil Group\" \"Multiple MSG Flag\"         \n [7] \"Flag - MSG Values\"          \"Flag - Fields\"             \n [9] \"Capability Class (FM5 CAP)\" \"Soil Temp. Regime\"         \n[11] \"Soil Modifier\"              \"Soil Slope\"                \n[13] \"Soil Name\"                  \"Drainage\"                  \n[15] \"Modifier\"                   \"Texture\"                   \n[17] \"Flooding\"                   \"Lime\"                      \n[19] \"Rotation\"                   \"Corn Yield (ton/acre)\"     \n[21] \"Hay Yield (ton/acre)\"       \"Change\"                    \n[23] \"TDN (ton/acre)\"             \"Index (TDN)\"               \n\n\n\n\nShow the code\n# Ensure 'Multiple MSG Flag' includes 'No' as a factor level correctly\ndata$`Multiple MSG Flag` &lt;- factor(data$`Multiple MSG Flag`, levels = c(\"Yes\", \"No\"))\n\n# Now safely replace NA values with \"No\"\ndata$`Multiple MSG Flag`[is.na(data$`Multiple MSG Flag`)] &lt;- \"No\"\n\n# Verify changes to ensure \"No\" is now included and NAs are handled\ntable(data$`Multiple MSG Flag`)\n\n\n\n Yes   No \n 451 8062 \n\n\n\n\nShow the code\nlibrary(plotly)\n\n\nWarning: package 'plotly' was built under R version 4.3.3\n\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n\nShow the code\n# Create ggplot for Corn Yield\np_corn &lt;- ggplot(data, aes(x = `Multiple MSG Flag`, y = `Corn Yield (ton/acre)`)) +\n  geom_boxplot(fill = \"blue\") +\n  labs(title = \"Corn Yield by Multiple MSG Flag\", x = \"Multiple MSG Flag\", y = \"Corn Yield\")\n\n# Convert to plotly\nplotly_corn &lt;- ggplotly(p_corn)\n\n# Display the plots\nplotly_corn\n\n\n\n\n\n\n\n\nShow the code\n# Create ggplot for Hay Yield\np_hay &lt;- ggplot(data, aes(x = `Multiple MSG Flag`, y = `Hay Yield (ton/acre)`)) +\n  geom_boxplot(fill = \"green\") +\n  labs(title = \"Hay Yield by Multiple MSG Flag\", x = \"Multiple MSG Flag\", y = \"Hay Yield\")\n\nplotly_hay &lt;- ggplotly(p_hay)\nplotly_hay"
  },
  {
    "objectID": "finalproj.html#research-question-2-correlation-between-the-drainage-of-the-soil-and-the-presence-of-flooding",
    "href": "finalproj.html#research-question-2-correlation-between-the-drainage-of-the-soil-and-the-presence-of-flooding",
    "title": "Final Project",
    "section": "Research Question 2 : Correlation between the drainage of the soil and the presence of flooding?",
    "text": "Research Question 2 : Correlation between the drainage of the soil and the presence of flooding?\nThe Pearson correlation coefficient between the ‘Drainage’ and ‘Flooding’ columns in your dataset is approximately 0.625, indicating a moderate positive correlation. This suggests that as drainage characteristics of the soil increase (presumably indicating better drainage capacity), the occurrence of flooding also increases. This result might seem counterintuitive at first glance; however, it could indicate that areas classified with better drainage are also those where water accumulates quickly, thus making them prone to flooding under certain conditions.\n\nInterpretation and Next Steps: Statistical Significance: The p-value is less than 0.05 (actually, it’s much smaller than that), indicating that the correlation is statistically significant, and the likelihood that this correlation is due to random chance is very low. Practical Significance: While statistically significant, the strength of the correlation (moderate) suggests that while there is a relationship, other factors also significantly influence the presence of flooding. It’s important to consider these factors in any risk assessment or land use planning. Further Analysis: Investigating how other variables interact with ‘Drainage’ and ‘Flooding’ could provide deeper insights. For instance, soil texture, slope, and capability class could be influencing how quickly water is absorbed or runs off, affecting flooding.\n\nThese plots are key to understanding which predictors are most influential in modeling the outcome (in your case, flooding).\nInterpretation of the Variable Importance Plots: %IncMSE: This plot shows the increase in Mean Squared Error (MSE) of the model when each variable is randomly shuffled. A higher value indicates that the model relies more on that variable for prediction, meaning the variable is more important. According to your graph, ‘Drainage’ seems to be the most important predictor, followed by ‘Capability_Class’ and ‘Texture’. IncNodePurity: This measure is based on the total decrease in node impurities from splitting on the variable, averaged over all trees. Node impurity is typically measured by the RSS (regression) or Gini impurity (classification). In this graph, ‘Capability_Class’ contributes most to node purity, followed by ‘Drainage’ and ‘Texture’. This suggests that ‘Capability_Class’ is particularly effective at creating homogeneous nodes, likely due to its role in determining soil usability.\n#Investigating interaction effects between predictors like ‘Drainage’ and ‘Texture’ in the context of their impact on flooding can provide deeper insights into how these variables jointly influence the outcome. In R, you can include interaction terms directly in your model formula to study these effects. Here, we’ll look at two approaches: using multiple regression to evaluate the statistical significance of the interaction, and using a Random Forest model to assess the predictive power when interactions are considered.\nR Code for Multiple Regression with Interaction Terms We’ll modify the linear regression model to include an interaction term between ‘Drainage’ and ‘Texture’. This will allow us to see if the effect of one variable on flooding depends on the level of the other variable.\n\nInterpretation of the Regression Output Coefficients: Intercept (-0.349160): This represents the baseline value of flooding when both ‘Drainage’ and ‘Texture’ are at their reference levels (typically zero in numerical coding). Drainage (0.229149): This coefficient indicates that for each unit increase in drainage (without considering the impact of texture), flooding increases by approximately 0.229 units, holding other factors constant. Texture (0.294757): Similarly, for each unit increase in texture, flooding increases by approximately 0.295 units, holding other factors constant.\nDrainage:Texture Interaction (-0.020150): The negative interaction term suggests that the combined effect of ‘Drainage’ and ‘Texture’ on flooding is less than the sum of their individual effects. In other words, higher levels of one may slightly mitigate the influence of the other on flooding. Statistical Significance: All predictors, including the interaction term, are highly statistically significant (p &lt; 0.001), indicating strong evidence against the null hypothesis of no effect. Model Fit: Residual Standard Error (RSE) (0.6614): This measures the typical size of the residuals, and in your context, it implies that the standard deviation of the residuals is around 0.661 units. Multiple R-squared (0.5791): Approximately 57.91% of the variability in flooding is explained by the model, which is a decent level of explanatory power for natural science data. Adjusted R-squared (0.576): Slightly adjusted for the number of predictors, still indicating a good fit. Residuals: The spread of residuals suggests that while the model fits well for many observations (median close to zero), there are outliers and some predictions that deviate significantly from the actual values, as indicated by the min and max residuals.\nRandom Forest inherently considers interactions among features, but we can explicitly engineer an interaction feature to see how it influences model performance.\n\nInterpretation of the Random Forest Model Results: Model Performance: Mean of squared residuals (0.1969851): This value is considerably lower than the residual standard error from the linear regression model (0.6614), suggesting that the Random Forest model has better predictive accuracy. % Var explained (80.86%): A high percentage of variance explained indicates that the Random Forest model is effectively capturing the relationships and variability in the data. It explains more than 80% of the variance in flooding, which is significantly higher than the Multiple R-squared from the linear regression model (57.91%). Feature Importance: Drainage: Importance score of 115.0117, suggesting it’s a significant predictor of flooding. Texture: Importance score of 121.6274, slightly more influential than ‘Drainage’. Drainage_Texture_Interaction: With the highest importance score of 131.4226, this engineered feature seems to be the most significant predictor in the model. This underscores the value of including interaction terms explicitly, even in a model like Random Forest that inherently accounts for interactions among features. Comparison and Conclusion: Explanatory Power: The linear regression model provides clear coefficients that describe the relationship between each predictor and the outcome, including how the interaction term modifies these relationships. This is particularly useful for hypothesis testing and understanding the specific effects of changes in predictors. Predictive Accuracy: The Random Forest model outperforms the linear regression in terms of predictive accuracy, explaining a higher percentage of the variance in flooding and producing a lower mean squared residual. Feature Importance: Random Forest offers an advantage in evaluating the importance of features, including interactions, without needing a specific hypothesis about their effects.\n\nCross-Validation for Linear Regression and Random Forest\n\n\nThe cross-validation results for both the Linear Regression and Random Forest models provide a clear comparison of their performance:\nLinear Regression Model: RMSE (Root Mean Squared Error): 0.6608545 R-squared: 0.5871187 MAE (Mean Absolute Error): 0.4143339 These metrics indicate that the linear regression model explains about 58.71% of the variance in the data. The RMSE and MAE values provide a measure of the average error in the predictions.\nRandom Forest Model: RMSE: 0.4282331 R-squared: 0.8134469 MAE: 0.1653817 Optimal mtry: 2 The Random Forest model significantly outperforms the Linear Regression in all the metrics. It explains approximately 81.34% of the variance in the data, and both its RMSE and MAE are lower, indicating more accurate predictions.\n\nThe diagnostic plots for your linear regression model provide valuable insights into how well the model meets the assumptions necessary for optimal performance. Here’s an interpretation of each plot:\n\n\nResiduals vs Fitted This plot helps check for non-linearity and heteroscedasticity (unequal variance of residuals).\n\nObservations: The residuals do not appear to display any clear pattern, which is good for linearity. However, there is a slight “fanning” effect where the spread of residuals increases with fitted values, suggesting potential heteroscedasticity. Action: Consider transformations of the dependent variable or use heteroscedasticity-consistent standard errors if this model will be used for inferential purposes. 2. Normal Q-Q This plot shows if the residuals are normally distributed—a key assumption of linear regression.\nObservations: Most points lie on the line, but there are deviations at the tails (both lower and upper ends), indicating slight departures from normality. Action: This is generally not severe unless very precise estimates are required. For more robustness, consider using non-parametric bootstrapping techniques to estimate standard errors. 3. Scale-Location (or Spread-Location) This plot checks if residuals are spread equally along the ranges of predictors (homoscedasticity).\nObservations: The red line (a loess fit) shows a trend, which suggests that residuals have non-constant variance across the range of fitted values. Action: This supports the earlier suggestion of possible heteroscedasticity. Transformations or robust regression methods might be needed. 4. Residuals vs Leverage This plot helps to identify influential cases that might have an unduly large effect on the model estimate.\nObservations: Most data points have low leverage, but there are a few points well outside the Cook’s distance lines (notably the points labeled 7280 and 4430). Action: Investigate these points further to determine if they are outliers or influential points due to data entry errors or other reasons. Consider removing or adjusting these points if they are deemed to be errors. Conclusion The diagnostic plots indicate that while the model does not suffer from severe issues, there are indications of potential heteroscedasticity and some influence from outliers or high-leverage points."
  },
  {
    "objectID": "finalproj.html#research-question-3-how-does-the-presence-of-multiple-mineral-soil-group-multiple-msg-flag-affect-corn-and-hay-yield-across-different-counties",
    "href": "finalproj.html#research-question-3-how-does-the-presence-of-multiple-mineral-soil-group-multiple-msg-flag-affect-corn-and-hay-yield-across-different-counties",
    "title": "Final Project",
    "section": "Research Question 3: How does the presence of multiple mineral soil group (Multiple MSG Flag) affect corn and hay yield across different counties?",
    "text": "Research Question 3: How does the presence of multiple mineral soil group (Multiple MSG Flag) affect corn and hay yield across different counties?\nThe aim of this question is to analyze the influence of the Multiple MSG Flag on the yield of corn and hay, and to determine if there are significant differences in yield based on this soil classification.\nVariables:\nDependent Variables: Corn yield and hay yield (measured in tons per acre). Independent Variable: Multiple MSG Flag (indicating presence of multiple mineral soil groups absence).\n\n\nShow the code\n# Load necessary libraries\nlibrary(readr)  # For reading CSV files\nlibrary(dplyr)  # For data manipulation\nlibrary(ggplot2)  # For creating visualizations\n\n# dataset\ndata = read_csv(\"C:\\\\Users\\\\Dell\\\\Downloads\\\\NYSERDA_2023_Soils_Data_for_use_in_the_Large-Scale_Renewables_and_NY-Sun_Programs.csv\")\n\n\nRows: 8513 Columns: 24\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (16): County, County_MAPSYM, MAPSYM, Multiple MSG Flag, Flag - Fields, C...\ndbl  (8): MUKEY, Default Mineral Soil Group, Flag - MSG Values, Rotation, Co...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nShow the code\n# Check all column names in the dataset\ncolnames(data)\n\n\n [1] \"County\"                     \"County_MAPSYM\"             \n [3] \"MAPSYM\"                     \"MUKEY\"                     \n [5] \"Default Mineral Soil Group\" \"Multiple MSG Flag\"         \n [7] \"Flag - MSG Values\"          \"Flag - Fields\"             \n [9] \"Capability Class (FM5 CAP)\" \"Soil Temp. Regime\"         \n[11] \"Soil Modifier\"              \"Soil Slope\"                \n[13] \"Soil Name\"                  \"Drainage\"                  \n[15] \"Modifier\"                   \"Texture\"                   \n[17] \"Flooding\"                   \"Lime\"                      \n[19] \"Rotation\"                   \"Corn Yield (ton/acre)\"     \n[21] \"Hay Yield (ton/acre)\"       \"Change\"                    \n[23] \"TDN (ton/acre)\"             \"Index (TDN)\"               \n\n\n\nlibrary(plotly)\n\n# Create ggplot for Corn Yield\np_corn &lt;- ggplot(data, aes(x = `Multiple MSG Flag`, y = `Corn Yield (ton/acre)`)) +\n  geom_boxplot(fill = \"blue\") +\n  labs(title = \"Corn Yield by Multiple MSG Flag\", x = \"Multiple MSG Flag\", y = \"Corn Yield\")\n\n# Convert to plotly\nplotly_corn &lt;- ggplotly(p_corn)\n\n# Display the plots\nplotly_corn\n\n\n\n\n\nBox Plot Analysis for “Yes” Flag in Corn Yield Minimum Yield: The minimum yield recorded is 0, indicating that there were instances where no corn was produced. First Quartile (Q1): The first quartile is at 9.00 tons/acre, which means that 25% of the yields are below this value. This lower quartile suggests a significant number of plots produced yields less than 9 tons per acre. Median (Q2): The median yield is 10.50 tons/acre, indicating that half of the yields are below this value and half are above. This median is fairly low compared to the maximum, suggesting a skewed distribution with a clustering of yields around the lower mid-range. Third Quartile (Q3): The third quartile is at 18.48 tons/acre, showing that 75% of the yields are below this level. This wider spread between the median and Q3 indicates variability in the higher yield range. Maximum Yield: The maximum yield observed is 24.00 tons/acre, significantly lower than the maximum yields observed in the “No” condition, suggesting that while the “Yes” flag might include high-yield scenarios, its peak potential is less than that of the “No” flag scenarios.\nMinimum Yield: The minimum yield starts at 0, similar to the “Yes” category, indicating that there were also no-yield instances under the “No” condition. First Quartile (Q1): Again, it is close to zero which suggests a significant portion of the data also produced very low yields. Median Yield: The median is noticeably lower than in the “Yes” condition, at about 12.00 tons/acre. Third Quartile (Q3): Positioned around 13.68 tons/acre, which is lower than the third quartile in the “Yes” category, indicating less variability in the higher yield segment under the “No” condition. Maximum Yield: The maximum yield extends up to about 40.56 tons/acre, which is similar to the “Yes” condition, indicating the highest yields are comparable between the two groups. However, these very high yields are outliers, as shown by the points above the upper fence (31.50 tons/acre).\n\n# Create ggplot for Hay Yield\np_hay &lt;- ggplot(data, aes(x = `Multiple MSG Flag`, y = `Hay Yield (ton/acre)`)) +\n  geom_boxplot(fill = \"green\") +\n  labs(title = \"Hay Yield by Multiple MSG Flag\", x = \"Multiple MSG Flag\", y = \"Hay Yield\")\n\nplotly_hay &lt;- ggplotly(p_hay)\nplotly_hay\n\n\n\n\n\nBox Plot Analysis for Hay Yield For “Yes”: Minimum Yield: The minimum yield starts at 0, indicating that some plots did not produce any hay. First Quartile (Q1): The first quartile is around 0 as well, suggesting that at least 25% of the plots produced very little to no hay. Median Yield: The median yield is 1.63 tons/acre, indicating that half of the plots under the “Yes” condition produced this amount or less. This is a relatively low median, suggesting moderate production overall but lower than might be expected. Third Quartile (Q3): The third quartile at 3.35 tons/acre shows that 75% of the plots produced 3.35 tons/acre or less. This wider spread from the median to Q3 indicates variability in the higher production range. Maximum Yield: The maximum yield reported is 4.62 tons/acre, which is quite low compared to corn yield outliers. This suggests that even the highest-producing hay plots under the “Yes” flag aren’t exceedingly productive.\nBox Plot Analysis for “No” Flag in Hay Yield Minimum Yield: The minimum yield starts at 0, similar to the “Yes” condition, indicating that some plots under this condition also did not produce any hay. First Quartile (Q1): Positioned at 0.96 tons/acre, this marks where 25% of the yields fall below. This value suggests a base level of production that is slightly higher than the absolute minimum, indicating a better performance in the lower yield quartile compared to the “Yes” condition. Median Yield: The median yield is 1.95 tons/acre, which is exactly the same as in the “Yes” condition. This indicates a similar central tendency in terms of yield across both soil group conditions. Third Quartile (Q3): The third quartile is 2.68 tons/acre, showing that 75% of the data fall below this yield. The range between the first and third quartile is narrower compared to the “Yes” condition, indicating less variability in the higher yield ranges. Upper Fence: The upper fence at 4.02 tons/acre sets the boundary for what is considered normal yield variation; yields beyond this are treated as outliers. Maximum Yield: The maximum observed yield is 8.04 tons/acre, which is a significant outlier and suggests that a few plots can produce substantially more than typical plots under the “No” condition.\nLinear Regression Models :\n\nModel for Corn Yield:\nCoefficients: The intercept was significant at 12.5918 tons/acre (p &lt; 2e-16), indicating the average yield when the flag is “Yes”. The coefficient for “No” was -2.8169 (p &lt; 2e-16), indicating a significant reduction in yield when the flag is “No”. Model Fit: The model explained only 1.019% of the variance in yield (R-squared = 0.01019), suggesting that other factors besides the MSG flag are more influential in determining corn yield.\n\n\nModel for Hay Yield:\nCoefficients: The intercept for “Yes” was 2.31247 tons/acre (p &lt; 2e-16), with the “No” flag associated with a decrease in yield by 0.43985 tons/acre (p &lt; 9.6e-14). Model Fit: The R-squared value was 0.006494, indicating that the model explains a very small portion of the variance in hay yields.\nThe presence of multiple mineral soil groups (Multiple MSG Flag) positively affects both corn and hay yields. Our analysis shows that fields with Multiple mineral soil groups tend to yield higher compared to fields without such diversity. Specifically, regression analysis revealed significant increases in yield for areas with multiple soil groups: corn yields decrease by about 2.817 tons per acre and hay yields by about 0.440 tons per acre in the absence of soil diversity.\nBoth models show that the “Multiple MSG Flag” being “No” is associated with a decrease in yield for both corn and hay compared to when the flag is “Yes”. However, the effect is stronger and more pronounced for corn. Both models explain a very small fraction of the variability in yields, which suggests that there are many other factors affecting yield that are not captured by this variable alone."
  },
  {
    "objectID": "finalproj.html#references",
    "href": "finalproj.html#references",
    "title": "Final Project",
    "section": "References:",
    "text": "References:\nDataset :\nFor reference click this link to redirected to the dataset:\nhttps://data.ny.gov/Energy-Environment/NYSERDA-2023-Soils-Data-for-use-in-the-Large-Scale/dayw-t2bj/about_data\nOverview of the dataset: https://data.ny.gov/api/views/dayw-t2bj/files/eda3c994-48d7-4f59-8ee4-5e8c7a022b83?download=true&filename=NYSERDA_Soils2023_Overview.pdf\nDescription for the columns: https://data.ny.gov/api/views/dayw-t2bj/files/a9db1226-91d1-40f6-ae9c-ee4a34b20b67?download=true&filename=NYSERDA_Soils2023_DataDictionary.pdf\nhttps://www.mdpi.com/2073-4395/12/11/2613\nhttps://journalijpss.com/index.php/IJPSS/article/view/3685"
  },
  {
    "objectID": "finalproj.html#section",
    "href": "finalproj.html#section",
    "title": "Final Project",
    "section": "",
    "text": "Average Corn Yield by County (First Graph):\nThis bar graph represents the average corn yield for various counties, arranged in descending order. Each bar reflects the yield of corn in tons per acre for a specific county.\nKey Observations:\nYield Variability: There is a significant variability in corn yields among counties. The graph shows a stark contrast between the highest-yielding counties and those at the lower end. Counties with yields over 10 tons per acre contrast sharply with those that barely produce 1 ton per acre.\nGeographical Clustering: The highest-yielding counties are grouped on the left side of the graph. This clustering might suggest geographical or climatic advantages, or it could reflect areas with more advanced farming techniques and better soil management practices.\nAverage Hay Yield by County (Second Graph):\nSimilar to the first, this graph displays the average hay yield in tons per acre across different counties, arranged in descending order.\nKey Observations:\nMore Uniform Yields: Compared to corn, hay yields across counties show a more uniform distribution. The highest yields are not as extreme, and the decrease across counties appears more gradual.\nLower Maximum Yields: The highest hay yields are significantly lower than the highest corn yields, which might reflect different market values, crop demands, or inherent differences in how these crops are cultivated and harvested."
  },
  {
    "objectID": "finalprojcode.html#section",
    "href": "finalprojcode.html#section",
    "title": "Final Project Code",
    "section": "",
    "text": "{r} library(dplyr) library(ggplot2) library(plotly)"
  }
]